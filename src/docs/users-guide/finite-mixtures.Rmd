# Finite Mixtures  {#mixture-modeling.chapter}

Finite mixture models of an outcome assume that the outcome is drawn
from one of several distributions, the identity of which is controlled
by a categorical mixing distribution.  Mixture models typically have
multimodal densities with modes near the modes of the mixture
components.  Mixture models may be parameterized in several ways, as
described in the following sections.  Mixture models may be used
directly for modeling data with multimodal distributions, or they may
be used as priors for other parameters.

## Relation to Clustering {#clustering-mixture.section}

Clustering models, as discussed in the [clustering
chapter](#clustering.chapter), are just a particular class of mixture
models that have been widely applied to clustering in the engineering
and machine-learning literature.  The normal mixture model discussed
in this chapter reappears in multivariate form as the statistical
basis for the $K$-means algorithm; the latent Dirichlet allocation
model, usually applied to clustering problems, can be viewed as a
mixed-membership multinomial mixture model.


## Latent Discrete Parameterization

One way to parameterize a mixture model is with a latent categorical
variable indicating which mixture component was responsible for the
outcome. For example, consider $K$ normal distributions with locations
$\mu_k \in \mathbb{R}$ and scales $\sigma_k \in (0,\infty)$.  Now consider
mixing them in proportion $\lambda$, where $\lambda_k \geq 0$ and
$\sum_{k=1}^K \lambda_k = 1$ (i.e., $\lambda$ lies in the unit $K$-simplex).
For each outcome $y_n$ there is a latent variable $z_n$ in
$\{ 1,\ldots,K \}$ with a categorical distribution parameterized
by $\lambda$,

$$
z_n \sim \mathsf{Categorical}(\lambda).
$$

The variable $y_n$ is distributed according to the parameters
of the mixture component $z_n$,
$$
y_n \sim \mathsf{normal}(\mu_{z[n]},\sigma_{z[n]}).
$$

This model is not directly supported by Stan because it involves
discrete parameters $z_n$, but Stan can sample $\mu$ and $\sigma$
by summing out the $z$ parameter as described in the next section.


## Summing out the Responsibility Parameter

To implement the normal mixture model outlined in the previous
section in Stan, the discrete parameters can be summed out of the
model. If $Y$ is a mixture of $K$ normal distributions with
locations $\mu_k$ and scales $\sigma_k$ with mixing proportions
$\lambda$ in the unit $K$-simplex, then
$$
p_Y(y | \lambda, \mu, \sigma)
\ = \
\sum_{k=1}^K \lambda_k \, \mathsf{normal}(y \, | \, \mu_k, \sigma_k).
$$


### Log Sum of Exponentials: Linear Sums on the Log Scale {-}

The log sum of exponentials function is used to define mixtures on the
log scale.  It is defined for two inputs by

$$
\mbox{log\_sum\_exp}(a, b) = \log (\exp(a) + \exp(b)).
$$

If $a$ and $b$ are probabilities on the log scale, then $\exp(a) +
\exp(b)$ is their sum on the linear scale, and the outer log converts
the result back to the log scale; to summarize, log_sum_exp does
linear addition on the log scale.   The reason to use Stan's built-in
`log_sum_exp`  function is that it can prevent underflow and overflow
in the exponentiation, by calculating the result as

$$
\log\left( \exp(a) + \exp(b)\right)
= c
  + \log \left( \exp(a - c) + \exp(b - c) \right),
$$

where $c = \max(a, b)$.  In this evaluation, one of the terms, $a - c$
or $b - c$, is zero and the other is negative, thus eliminating the
possibility of overflow or underflow in the leading term while
extracting the most arithmetic precision possible by pulling the
`max(a, b)` out of the log-exp round trip.

For example, the mixture of $\mathsf{normal}(-1, 2)$ with
$\mathsf{normal}(3, 1)$, with mixing proportion $\lambda =
[0.3,0.7]^{\top}$, can be implemented in Stan as follows.

```
parameters {
  real y;
}
model {
  target += log_sum_exp(log(0.3) + normal_lpdf(y | -1, 2),
                        log(0.7) + normal_lpdf(y | 3, 1));
}
```

The log probability term is derived by taking

$$
\log p(y | \lambda,\mu,\sigma)  = \log\!\left( 0.3 * \mathsf{normal}(y|-1,2) \, + \,
  0.7 *
  \mathsf{normal}(y|3,1) \, \right)
\\
 = \log(
                 \exp(\log(0.3 * \mathsf{normal}(y|-1,2))) 
                 + \exp(\log(0.7 * \mathsf{normal}(y|3,1))) )
\\
 = \mbox{log\_sum\_exp}(
                         \log(0.3) + \log \mathsf{normal}(y|-1,2),
                         \log(0.7) + \log \mathsf{normal}(y|3,1) ).
$$


### Dropping uniform mixture ratios {-}

If a two-component mixture has a mixing ratio of 0.5, then the mixing
ratios can be dropped, because

```
neg_log_half = -log(0.5);
for (n in 1:N)
  target += log_sum_exp(neg_log_half
                        + normal_lpdf(y[n] | mu[1], sigma[1]),
                        neg_log_half
			+ normal_lpdf(y[n] | mu[2], sigma[2]));
```

then the $-\log 0.5$ term isn't contributing to the proportional
density, and the above can be replaced with the more efficient version

```
for (n in 1:N)
  target += log_sum_exp(normal_lpdf(y[n] | mu[1], sigma[1]),
                        normal_lpdf(y[n] | mu[2], sigma[2]));
```

The same result holds if there are $K$ components and the mixing
simplex $\lambda$ is symmetric, i.e.,

$$
\lambda = \left( \frac{1}{K},   \ldots, \frac{1}{K} \right).
$$

The result follows from the identity

$$
\mbox{log\_sum\_exp}(c + a, c + b)
\ = \
c + \mbox{log\_sum\_exp(a, b)}
$$

and the fact that adding a constant $c$ to the log density accumulator
has no effect because the log density is only specified up to an
additive constant in the first place.  There is nothing specific to
the normal distribution here; constants may always be dropped from the
target.


### Recovering posterior mixture proportions {-}

The posterior $p(z_n \mid y_n, \mu, \sigma)$ over the mixture indicator $z_n
\in 1:K$ is often of interest as $p(z_n = k \mid y, \mu, \sigma)$ is the
posterior probability that that observation $y_n$ was generated by
mixture component $k$.  The posterior can be computed via Bayes's rule,
$$
\begin{array}{rcl}
\mbox{Pr}(z_n = k \mid y_n, \mu, \sigma, \lambda)
& \propto &
p(y_n \mid z_n = k, \mu, \sigma) p(z_n = k \mid \lambda)
\\
& = & \mathsf{normal}(y_n \mid \mu_k, \sigma_k) \cdot \lambda_k.
\end{array}
$$


The normalization can be done via summation, because $z_n \in 1{:}K$ only
takes on finitely many values.  In detail,
$$
p(z_n = k \mid y_n, \mu, \sigma, \lambda)
\ = \
\frac{p(y_n \mid z_n = k, \mu, \sigma) \cdot p(z_n = k \mid \lambda)}
     {\sum_{k' = 1}^K p(y_n \mid z_n = k', \mu, \sigma)
                    \cdot p(z_n = k' \mid \lambda)}.
$$

On the log scale, the normalized probability is computed as
$$
\begin{array}{l}
\log \mbox{Pr}(z_n = k \mid y_n, \mu, \sigma, \lambda)
\\
\mbox { } \ \ \ = \log p(y_n \mid z_n = k, \mu, \sigma)
  + \log \mbox{Pr}(z_n = k \mid \lambda)
  - \mathrm{log\_sum\_exp}_{k' = 1}^K
      \log p(y_n \mid z_n = k', \mu, \sigma)
      + \log p(z_n = k' \mid \lambda).
\end{array}
$$
This can be coded up directly in Stan; the change-point model in the
[change point section](#change-point.section) provides an example.

### Estimating Parameters of a Mixture {-}

Given the scheme for representing mixtures, it may be moved to an
estimation setting, where the locations, scales, and mixture
components are unknown.  Further generalizing to a number of mixture
components specified as data yields the following model.

```
data {
  int<lower=1> K;          // number of mixture components
  int<lower=1> N;          // number of data points
  real y[N];               // observations
}
parameters {
  simplex[K] theta;          // mixing proportions
  ordered[K] mu;             // locations of mixture components
  vector<lower=0>[K] sigma;  // scales of mixture components
}
model {
  vector[K] log_theta = log(theta);  // cache log calculation
  sigma ~ lognormal(0, 2);
  mu ~ normal(0, 10);
  for (n in 1:N) {
    vector[K] lps = log_theta;
    for (k in 1:K)
      lps[k] += normal_lpdf(y[n] | mu[k], sigma[k]);
    target += log_sum_exp(lps);
  }
}
```

The model involves `K` mixture components and `N` data
points. The mixing proportion parameter `theta` is declared to be
a unit $K$-simplex, whereas the component location parameter `mu`
and scale parameter `sigma` are both defined to be
`K`-vectors.

The location parameter `mu` is declared to be an ordered vector
in order to identify the model.  This will not affect inferences that
do not depend on the ordering of the components as long as the prior
for the components `mu[k]` is symmetric, as it is here (each
component has an independent $\mathsf{normal}(0, 10)$ prior).  It
would even be possible to include a hierarchical prior for the components.

The values in the scale array `sigma` are constrained to be
non-negative, and have a weakly informative prior given in the model
chosen to avoid zero values and thus collapsing components.

The model declares a local array variable `lps` to be size
`K` and uses it to accumulate the log contributions from the
mixture components.  The main action is in the loop over data points
`n`.  For each such point, the log of $\theta_k *
\mathsf{normal}(y_n \, | \, \mu_k,\sigma_k)$ is calculated and added to the
array `lpps`.  Then the log probability is incremented with the log
sum of exponentials of those values.

## Vectorizing Mixtures

There is (currently) no way to vectorize mixture models at the
observation level in Stan.  This section is to warn users away from
attempting to vectorize naively, as it results in a different model.
A proper mixture at the observation level is defined as follows, where
we assume that `lambda`, `y[n]`, `mu[1], mu[2]`, and
`sigma[1], sigma[2]` are all scalars and `lambda` is between
0 and 1.

```
for (n in 1:N) {
  target += log_sum_exp(log(lambda)
                          + normal_lpdf(y[n] | mu[1], sigma[1]),
                        log1m(lambda)
                          + normal_lpdf(y[n] | mu[2], sigma[2]));
```

or equivalently

```
for (n in 1:N)
  target += log_mix(lambda,
                    normal_lpdf(y[n] | mu[1], sigma[1]),
                    normal_lpdf(y[n] | mu[2], sigma[2]));
```

This definition assumes that each observation $y_n$ may have arisen
from either of the mixture components. The density is
$$
p(y \, | \, \lambda, \mu, \sigma)
= \prod_{n=1}^N (\lambda * \mathsf{normal}(y_n \, | \, \mu_1, \sigma_1)
                 + (1 - \lambda) * \mathsf{normal}(y_n \, | \, \mu_2, \sigma_2).
$$

Contrast the previous model with the following (erroneous) attempt to
vectorize the model.

```
target += log_sum_exp(log(lambda)
                        + normal_lpdf(y | mu[1], sigma[1]),
                      log1m(lambda)
                        + normal_lpdf(y | mu[2], sigma[2]));
```

or equivalently,

```
target += log_mix(lambda,
                  normal_lpdf(y | mu[1], sigma[1]),
                  normal_lpdf(y | mu[2], sigma[2]));
```

This second definition implies that the entire sequence $y_1, \ldots, y_n$ of
observations comes form one component or the other, defining a
different density,
$$
p(y \, | \, \lambda, \mu, \sigma)
= \lambda * \prod_{n=1}^N \mbox{normal}(y_n \, | \, \mu_1, \sigma_1)
+ (1 - \lambda) * \prod_{n=1}^N \mbox{normal}(y_n \, | \, \mu_2, \sigma_2).
$$


## Inferences Supported by Mixtures {#mixture-inference.section}

In many mixture models, the mixture components are underlyingly
exchangeable in the model and thus not identifiable.  This arises if
the parameters of the mixture components have exchangeable priors and
the mixture ratio gets a uniform prior so that the parameters of the
mixture components are also exchangeable in the likelihood.

We have finessed this basic problem by ordering the parameters.  This
will allow us in some cases to pick out mixture components either
ahead of time or after fitting (e.g., male vs. female, or Democrat
vs.\ Republican).

In other cases, we do not care about the actual identities of the
mixture components and want to consider inferences that are
independent of indexes.  For example, we might only be interested
in posterior predictions for new observations.

### Mixtures with Unidentifiable Components {-}

As an example, consider the normal mixture from the previous section,
which provides an exchangeable prior on the pairs of parameters
$(\mu_1, \sigma_1)$ and $(\mu_2, \sigma_2)$,

$$
\begin{array}{rcl}
\mu_1, \mu_2 & \sim & \mathsf{normal}(0, 10)
\\[8pt]
\sigma_1, \sigma_2 & \sim & \mathsf{Halfnormal}(0, 10)
\end{array}
$$

The prior on the mixture ratio is uniform,

$$
\lambda \sim \mathsf{Uniform}(0, 1),
$$

so that with the likelihood

$$
p(y_n \, | \, \mu, \sigma)
= \lambda \, \mathsf{normal}(y_n \, | \, \mu_1, \sigma_1)
+ (1 - \lambda) \, \mathsf{normal}(y_n \, | \, \mu_2, \sigma_2),
$$

the joint distribution $p(y, \mu, \sigma, \lambda)$ is exchangeable
in the parameters $(\mu_1, \sigma_1)$ and $(\mu_2, \sigma_2)$ with
$\lambda$ flipping to $1 - \lambda$.^[Imposing a constraint such as $\theta < 0.5$ will resolve the symmetry, but fundamentally changes the model and its posterior inferences.]

### Inference under Label Switching {-}

In cases where the mixture components are not identifiable, it can be
difficult to diagnose convergence of sampling or optimization
algorithms because the labels will switch, or be permuted, in
different MCMC chains or different optimization runs.  Luckily,
posterior inferences which do not refer to specific component labels
are invariant under label switching and may be used directly.  This
subsection considers a pair of examples.

#### Predictive likelihood {-}

Predictive likelihood for a new observation $\tilde{y}$ given the
complete parameter vector $\theta$ will be

$$
p(\tilde{y} \, | \, y)
=
\int_{\theta}
p(\tilde{y} \, | \, \theta)
\, p(\theta | y)
\, \mbox{d}\theta.
$$

The normal mixture example from the previous section, with $\theta =
(\mu, \sigma, \lambda)$, shows that the likelihood returns the same
density under label switching and thus the predictive inference is
sound.  In Stan, that predictive inference can be done either by
computing $p(\tilde{y} \, | \, y)$, which is more efficient
statistically in terms of effective sample size, or simulating draws
of $\tilde{y}$, which is easier to plug into other inferences.  Both
approaches can be coded directly in the generated quantities block of
the program.  Here's an example of the direct (non-sampling) approach.

```
data {
  int<lower = 0> N_tilde;
  vector[N_tilde] y_tilde;
  ...
generated quantities {
  vector[N_tilde] log_p_y_tilde;
  for (n in 1:N_tilde)
    log_p_y_tilde[n]
      = log_mix(lambda,
                normal_lpdf(y_tilde[n] | mu[1], sigma[1])
                normal_lpdf(y_tilde[n] | mu[2], sigma[2]));
}
```

It is a bit of a bother afterwards, because the logarithm function
isn't linear and hence doesn't distribute through averages (Jensen's
inequality shows which way the inequality goes).  The right thing to
do is to apply `log_sum_exp` of the posterior draws of
`log_p_y_tilde`.  The average log predictive density is then
given by subtracting `log(N_new)`.


#### Clustering and similarity {-}

Often a mixture model will be applied to a clustering problem and
there might be two data items $y_i$ and $y_j$ for which there is a
question of whether they arose from the same mixture component.  If we
take $z_i$ and $z_j$ to be the component responsibility discrete
variables, then the quantity of interest is $z_i = z_j$, which can be
summarized as an event probability

$$
\mbox{Pr}[z_i = z_j \, | \, y]
=
\int_{\theta}
\frac{\sum_{k=0}^1 p(z_i=k, z_j = k, y_i, y_j \, | \, \theta)}
     {\sum_{k=0}^1 \sum_{m=0}^1 p(z_i = k, z_j = m, y_i, y_j \, | \,
       \theta)}
\
p(\theta \, | \, y)
\
\mbox{d}\theta.
$$

As with other event probabilities, this can be calculated in the
generated quantities block either by sampling $z_i$ and $z_j$ and
using the indicator function on their equality, or by computing the
term inside the integral as a generated quantity.  As with predictive
likelihood, working in expectation is more statistically efficient than
sampling.

## Zero-Inflated and Hurdle Models {#zero-inflated.section}

Zero-inflated and hurdle models both provide mixtures of a Poisson and
Bernoulli probability mass function to allow more flexibility in
modeling the probability of a zero outcome.  Zero-inflated models, as
defined by @Lambert:1992, add additional probability mass to
the outcome of zero.  Hurdle models, on the other hand, are formulated
as pure mixtures of zero and non-zero outcomes.

Zero inflation and hurdle models can be formulated for discrete
distributions other than the Poisson.  Zero inflation does not work
for continuous distributions in Stan because of issues with
derivatives; in particular, there is no way to add a point mass to a
continuous distribution, such as zero-inflating a normal as a
regression coefficient prior.


### Zero Inflation {-}

Consider the following example for zero-inflated Poisson
distributions.  It uses a parameter `theta` here there is a
probability $\theta$ of drawing a zero, and a probability $1 - \theta$
of drawing from $\mathsf{Poisson}(\lambda)$ (now $\theta$ is being
used for mixing proportions because $\lambda$ is the traditional
notation for a Poisson mean parameter).  The probability function is
thus
$$
p(y_n|\theta,\lambda)
=
\left\{
\begin{array}{ll}
\theta + (1 - \theta) * \mathsf{Poisson}(0|\lambda) & \mbox{ if } y_n = 0, \mbox{ and}
\\[3pt]
(1-\theta) * \mathsf{Poisson}(y_n|\lambda) & \mbox{ if } y_n > 0.
\end{array}
\right.
$$

The log probability function can be implemented directly in Stan as follows.

```
data {
  int<lower=0> N;
  int<lower=0> y[N];
}
parameters {
  real<lower=0, upper=1> theta;
  real<lower=0> lambda;
}
model {
  for (n in 1:N) {
    if (y[n] == 0)
      target += log_sum_exp(bernoulli_lpmf(1 | theta),
                            bernoulli_lpmf(0 | theta)
                              + poisson_lpmf(y[n] | lambda));
    else
      target += bernoulli_lpmf(0 | theta)
                  + poisson_lpmf(y[n] | lambda);
  }
}
```

The `log_sum_exp(lp1,lp2)` function adds the log probabilities
on the linear scale; it is defined to be equal to `log(exp(lp1) +
  exp(lp2))`, but is more arithmetically stable and faster.

#### Optimizing the zero-inflated Poisson model {-}

The code given above to compute the zero-inflated Poisson
redundantly calculates all of the Bernoulli terms and also
`poisson_lpmf(0 | lambda)` every time the first condition
body executes.  The use of the redundant terms is conditioned on
`y`, which is known when the data are read in.  This allows
the transformed data block to be used to compute some more convenient
terms for expressing the log density each iteration.

The number of zero cases is computed and handled separately.
Then the nonzero cases are collected into their own array for
vectorization.  The number of zeros is required to declare
`y_nonzero`, so it must be computed in a function.

```
functions {
  int num_zeros(int[] y) {
    int sum = 0;
    for (n in 1:size(y))
      sum += (y[n] == 0);
    return sum;
  }
}
...
transformed data {
  int<lower = 0> N_zero = num_zeros(y);
  int<lower = 1> y_nonzero[N - N_zero];
  int N_nonzero = 0;
  for (n in 1:N) {
    if (y[n] == 0) continue;
    N_nonzero += 1;
    y_nonzero[N_nonzero] = y[n];
  }
}
...
model {
  ...
   target
     += N_zero
          * log_sum_exp(bernoulli_lpmf(1 | theta),
                        bernoulli_lpmf(0 | theta)
                          + poisson_lpmf(0 | lambda));
   target += N_nonzero * bernoulli_lpmf(0 | theta);
   target += poisson_lpmf(y_nonzero | lambda);
...
```

The boundary conditions of all zeros and no zero outcomes is handled
appropriately;  in the vectorized case, if `y_nonzero` is empty,
`N_nonzero` will be zero, and the last two target increment
terms will add zeros.


### Hurdle Models {-}

The hurdle model is similar to the zero-inflated model, but more
flexible in that the zero outcomes can be deflated as well as
inflated.  The probability mass function for the hurdle likelihood is
defined by

$$
p(y|\theta,\lambda)
=
\begin{cases}
\ \theta & \mbox{if } y = 0, \mbox{ and}
\\
\ (1 - \theta)
  \
   \frac{\displaystyle \mathsf{Poisson}(y | \lambda)}
        {\displaystyle  1 - \mathsf{PoissonCDF}(0 | \lambda)}
& \mbox{if } y > 0,
\end{cases}
$$


where $\mathsf{PoissonCDF}$ is the cumulative distribution function for
the Poisson distribution.  The hurdle model is even more straightforward to
program in Stan, as it does not require an explicit mixture.

```
   if (y[n] == 0)
      1 ~ bernoulli(theta);
    else {
      0 ~ bernoulli(theta);
      y[n] ~ poisson(lambda) T[1, ];
    }
```

The Bernoulli statements are just shorthand for adding $\log \theta$
and $\log (1 - \theta)$ to the log density.  The `T[1,]` after the
Poisson indicates that it is truncated below at 1; see the [truncation
section](#truncation.section) for more about truncation and the
[Poisson regresison section](#poisson.section) for the specifics of
the Poisson CDF.  The net effect is equivalent to the direct
definition of the log likelihood.

```
   if (y[n] == 0)
      target += log(theta);
    else
      target += log1m(theta) + poisson_lpmf(y[n] | lambda)
                - poisson_lccdf(0 | lambda));
```

Julian King pointed out that because
$$
\log \left( 1 - \mathsf{PoissonCDF}(0 | \lambda) \right)
\ = \ \log \left( 1 - \mathsf{Poisson}(0 | \lambda) \right)
\ = \ \log(1 - \exp(-\lambda))
$$
the CCDF in the else clause can be replaced with a simpler expression.

```
      target += log1m(theta) + poisson_lpmf(y[n] | lambda)
                - log1m_exp(-lambda));
```

The resulting code is about 15% faster than the code with the CCDF.

This is an example where collecting counts ahead of time can also
greatly speed up the execution speed without changing the density.
For data size $N=200$ and parameters $\theta=0.3$ and $\lambda = 8$,
the speedup is a factor of 10; it will be lower for smaller $N$ and
greater for larger $N$; it will also be greater for larger $\theta$.

To achieve this speedup, it helps to have a function to count the
number of non-zero entries in an array of integers,

```
functions {
  int num_zero(int[] y) {
    int nz = 0;
    for (n in 1:size(y))
      if (y[n] == 0)
        nz += 1;
    return nz;
  }
}
```

Then a transformed data block can be used to store the sufficient
statistics,

```
transformed data {
  int<lower=0, upper=N> N0 = num_zero(y);
  int<lower=0, upper=N> Ngt0 = N - N0;
  int<lower=1> y_nz[N - num_zero(y)];
  {
    int pos = 1;
    for (n in 1:N) {
      if (y[n] != 0) {
        y_nz[pos] = y[n];
        pos += 1;
      }
    }
  }
}
```

The model block can then be reduced to three statements.

```
model {
  N0 ~ binomial(N, theta);
  y_nz ~ poisson(lambda);
  target += -Ngt0 * log1m_exp(-lambda);
}
```

The first statement accounts for the Bernoulli contribution to both
the zero and non-zero counts.  The second line is the Poisson
contribution from the non-zero counts, which is now vectorized.
Finally, the normalization for the truncation is a single line, so
that the expression for the log CCDF at 0 isn't repeated.  Also note
that the negation is applied to the constant `Ngt0`; whenever
possible, leave subexpressions constant because then gradients need
not be propagated until a non-constant term is encountered.


## Priors and Effective Data Size in Mixture Models

Suppose we have a two-component mixture model with mixing rate
$\lambda \in (0, 1)$.  Because the likelihood for the mixture
components is proportionally weighted by the mixture weights, the
effective data size used to estimate each of the mixture components
will also be weighted as a fraction of the overall data size.  Thus
although there are $N$ observations, the mixture components will be
estimated with effective data sizes of $\theta \, N$ and $(1 - \theta)
\, N$ for the two components for some $\theta \in (0, 1)$.  The
effective weighting size is determined by posterior responsibility,
not simply by the mixing rate $\lambda$.

### Comparison to Model Averaging {-}

In contrast to mixture models, which create mixtures at the
observation level, model averaging creates mixtures over the
posteriors of models separately fit with the entire data set.  In this
situation, the priors work as expected when fitting the models
independently, with the posteriors being based on the complete observed
data $y$.

If different models are expected to account for different
observations, we recommend building mixture models directly.  If the
models being mixed are similar, often a single expanded model will
capture the features of both and may be used on its own for
inferential purposes (estimation, decision making, prediction, etc.).
For example, rather than fitting an intercept-only regression and a
slope-only regression and averaging their predictions, even as a
mixture model, we would recommend building a single regression with
both a slope and an intercept.  Model complexity, such as having more
predictors than data points, can be tamed using appropriately
regularizing priors.  If computation becomes a bottleneck, the only
recourse can be model averaging, which can be calculated after fitting
each model independently (see @HoetingEtAl:1999 and
@GelmanEtAl:2013 for theoretical and computational details).

