\documentclass[article]{jss}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%% almost as usual
\author{{\bf\large Bob Carpenter}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Daniel Lee}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Marcus Brubaker}
        \\ Toyota Technological Institute
        %
    \And
        %
        {\bf\large Andrew Gelman}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Ben Goodrich}
        \\ Columbia University
        %
        \\[9pt]
        %
        {\bf\large Jiqiang Guo}
        \\ Columbia Univesity
        %
     \And
        %
        {\bf\large Matt Hoffman}
        \\ Adobe Research Labs
        %
        \\[9pt]
        %
        {\bf\large Michael Betancourt}
        \\ University College London
        %
        \\[9pt]
        %
        {\bf\large Peter Li}
        \\ Columbia University
}
\title{\proglang{Stan}: A Probabilistic Programming Language}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Bob Carpenter, Andrew Gelman, Matt Hoffman, 
  Daniel Lee, Ben Goodrich, Michael Betancourt, 
  Marcus Brubaker, Jiqiang Guo, Peter Li} %% comma-separated
\Plaintitle{Stan: A Probabilistic Programming Language} %% without formatting
\Shorttitle{Stan: A Probabilistic Programming Language} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{ \proglang{Stan} is a probabilistic programming language for
  specifying statistical models. A \proglang{Stan} program
  imperatively defines a log probability function over parameters
  conditioned on specified data and constants.  This may be contrasted
  with the \proglang{BUGS} language, in which a program declaratively
  defines a directed acyclic graphical model.

  Variables are declared by dimensionality and as to whether they
  represent data, transformed data, parameters, transformed
  parameters, or generated quantities; local variables are also
  supported.  In addition to unconstrained and constrained scalar,
  discrete, vector, matrix, and array types, there is a library of
  mathematical functions, probability-related functions, and matrix
  and linear algebra functions.  Statements, such as assignment,
  sampling, conditionals, and loops are executed imperatively in the
  order they are written.

  Full Bayesian inference is supported through Markov chain Monte
  Carlo (MCMC) methods such as the No-U-Turn sampler (NUTS).  
  Penalized maximum likelihood estimates
  are calculated using optimization methods such as BFGS.
  
  \proglang{Stan} programs are translated to \proglang{C++} and
  compiled.  The target \proglang{C++} code includes forward- and
  reverse-mode algorithmic differentiation to support samplers and
  optimizers requiring gradients, Hessians, Hessian-vector products,
  and other higher-order derivatives.  There is an extensive I/O
  library to deal with constrained variable input for data,
  initialization for parameters, and output for samples or point
  estimates.  There are also a range of tools to monitor convergence
  and mixing, summarize the posterior, perform Bayesian inference, and
  carry out posterior predictive checks.

  \proglang{Stan} can be called from the command line, through
  \proglang{R} using the \pkg{RStan} package, or through
  \proglang{Python} using the \pkg{PyStan} package.  All three
  interfaces support sampling or optimization-based inference and
  analysis.}

\Keywords{
  probabilistic program,
  Bayesian inference,
  algorithmic differentiation,
  \proglang{Stan}}
%
\Plainkeywords{
  probabilistic programming,
  Bayesian inference,
  algorithmic differentiation,
  Stan}


%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Bob Carpenter
  \\
  Department of Statistics
  \\ 
  Columbia University
  \\ 
  1255 Amsterdam Avenue
  \\ 
  New York, NY 10027
  \\
  U.S.A.
  \\
  E-mail: \email{carp@stat.columbia.edu}
  \\ 
  URL: \url{http://mc-stan.org/}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no \usepackage{Sweave.sty}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\section{TO-DO}

\begin{enumerate}
\item Convergence diagnostics?
\item more model examples?
\item description from C++ perspective?
\item what else do we want to make sure users don't miss?
\item optimization example?
\item future plans section?
\end{enumerate}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

\section[Why Stan?]{Why \proglang{Stan}?}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.

We did not set out to build \proglang{Stan} as it currently exists.
Our original goal was to apply full Bayesian inference to the sort of multilevel
generalized linear models discussed in Part~II of
\citep{GelmanHill:2007}, which are structured with grouped and
interacted predictors at multiple levels, hierarchical covariance
priors, nonconjugate coefficient priors, latent effects as in
item-response models, and varying output link functions and
distributions.

These models turned out to be a challenge for
existing general-purpose inference software.  A direct encoding in
\proglang{BUGS} \citep{LunnEtAl:2000,LunnEtAl:2009,LunnEtAl:2012} or
\proglang{JAGS} \citep{Plummer:2003} can grind these tools to a halt.
We began by adding custom vectorized logistic regressions to \proglang{JAGS}
using \proglang{C++} to overcome the cost of interpretation.  Although
this is much faster than looping in \proglang{JAGS}, it quickly became
clear that the root of the problem was the slow convergence of
conditional sampling when parameters were highly correlated 
in the posterior, as for time-series models and hierarchical models 
with interacted predictors.  We finally realized we needed a better sampler, 
not a more efficient implementation.

We briefly considered trying to tune proposals for a random-walk
Metropolis-Hastings sampler, but that seemed too problem specific and
not even necessarily possible without some kind of adaptation rather
than a global tuning of the proposals. 

We were hearing more about HMC (Hamiltonian Monte Carlo), which
appeared promising but was also problematic in that the Hamiltonian
dynamics simulation requires the gradient of the log posterior.
Although possible, computing these gradients by hand on a model-by-model basis
iis very tedious and error prone.  That is when we discovered
reverse-mode algorithmic differentiation, which, given a
templated \proglang{C++} function for the log posterior,
automatically computes a proper analytic gradient up to machine
precision accuracy in only a few multiples of the cost to evaluate the
log probability function itself.  We explored existing algorithmic
differentiation packages with open licenses such as \pkg{RAD}
\citep{Gay:2005} and its repackaging in the Sacado module of the
\pkg{Trilinos} toolkit, and the \pkg{CppAD} package of the COIN-OR
toolkit (see \citep{BellBurke:2008}).  Both packages, however, supported
few special functions (e.g., probability functions, log gamma,
inverse logit) or linear algebra operations (e.g., Cholesky
decompositions, matrix division), and neither are easily or modularly
extensible.

Consequently we built our own reverse-mode algorithmic differentiation package.
At that point, we ran into the problem that we could not just plug in
the probability functions from a package like \pkg{Boost} because they
weren't templated generally enough across all the arguments.  Rather
than pay the price of promoting floating point values to algorithmic
differentiation variables, we wrote our own fully templated
probability functions and other special functions.

Next, we integrated the \proglang{C++} package \pkg{Eigen} for matrix
operations and linear algebra functions.  \pkg{Eigen} makes extensive
use of expression templates for lazy evaluation and the CRTP
(curiously recurring template pattern) to implement concepts without
virtual function calls; \cite{VandevoordeJosuttis:2002} provide a 
complete description of template metaprogramming techniques. 
Unfortunately, we
ran into the same problem with Eigen as with the existing probability
libraries---it doesn't support mixed operations of algorithmic
differentiation variables and primitives like \code{double}.  Although
we initially began by promoting floating-point vectors to algorithmic
differentiation variables, we later completely rewrote our matrix
library to efficiently compute derivatives using rules described by
\cite{Giles:2008}.

At this point, we could fit models coded directly in \proglang{C++} on
top of the pre-release versions of the \proglang{Stan} API
(application programming interface).  Seeing how well this all worked,
we set our sights on the generality and ease of use of
\proglang{BUGS} and designed a modeling language in which
statisticians could write their models in familiar notation that could
be transformed to efficient \proglang{C++} code before being compiled into
an efficient executable program.  This paper is primarily about this
language and how it specifies models.
 
The next problem we ran into when we started implementing richer
models was variables with constrained support, such as positive variables, 
simplexes, and covariance matrices.  
Efficient implementation of these constraints
required the introduction of typed variables which automatically transformed to
unconstrained support with suitable adjustments to the log probability
from the log absolute Jacobian determinant of the inverse transforms.

Even with the prototype compiler generating models, we still faced a
major hurdle for ease of use. The efficiency of HMC is very sensitive
to two tuning parameters, the discretization interval (i.e., step
size) and the total simulation time (i.e., number of steps).  The interval
size parameter could be tuned during warm-up based on Metropolis
rejection rates, but the number of steps proved difficult to tune without
sacrificing the detailed balance of the sampler.  This led to the
development of the NUTS (No U-Turn) sampler
\citep{HoffmanGelman:2011}.  Roughly speaking, NUTS builds a tree
of possible samples by randomly simulating both forwards and backwards
in time until the combined trajectory turns back on itself.  Once the trajectory
has terminated, a new sample is drawn from the tree.

We thought we were home free at this point, but when we measured the
speed of some \proglang{BUGS} examples versus \proglang{Stan} we were
very disappointed.  \proglang{BUGS}'s very first example model, Rats,
ran more than an order of magnitude faster in \proglang{JAGS} than in
\proglang{Stan}.  Rats is a tough test case because the conjugate
priors and lack of posterior correlations make it an ideal candidate
for efficient Gibbs sampling.  

Realizing that we were doing redundant calculations, we wrote a
vectorized form of the normal distribution for multiple variates with
the same mean and scale, which sped things up a bit. At the same time,
we introduced some simple template metaprograms to remove the
calculation of constant terms in the log probability.  Performance was
lacking until we finally figured out how to both eliminate redundant 
calculations and partially evaluate the gradients using
a combination of expression templates and metaprogramming. 

When we attempted to fit a time-series model, we found that
normalizing the data to unit sample mean and variance sped up the fits
by an order of magnitude.  In hindsight this isn't surprising, as the 
performance of the numerical simulation in HMC scales with the variation 
of the parameter scales; fortunately HMC also provides a choice of metric,
or mass matrix, to compensate for variations in the log posterior.
In \proglang{Stan} 1.0 we introduced an adaptive 
diagonal metric into our HMC implementations that allowed the parameter 
scales to be normalized automatically, with the option of an adaptive dense 
metric and a full automated standardization of the posterior in \proglang{Stan} 2.0.

Because the dense metric performs only a global standardization, the
performance of \proglang{Stan} in models where the relative parameter
scales varies by location, such as hierarchical and latent models, can suffer.
RMHMC (Riemannian Manifold HMC) \citep{GirolamiCalderhead:2011}
introduces a location-dependent metric that can overcome these final
hurdles, and a full implementation within Stan is currently under development.

\section{Overview}

This section describes the use of \proglang{Stan} from the command
line for estimating (i.e., sampling from) the posterior of a Bayesian
model.

\subsection{Model for estimating a Bernoulli parameter}

Consider estimating the chance of success parameter for a Bernoulli
distribution based on a sequence of observed binary outcomes.  
Figure~\ref{bernoulli-model.fig} provides an implementation of such a
model in \proglang{Stan}.
%
\begin{figure}
\begin{Code}
data { 
  int<lower=0> N; 
  int<lower=0,upper=1> y[N];
} 
parameters {
  real<lower=0,upper=1> theta;
} 
model {
  theta ~ beta(1,1);  // prior
  for (n in 1:N) 
    y[n] ~ bernoulli(theta);  // likelihood
}
\end{Code}
\caption{Model for estimating a Bernoulli parameter.}\label{bernoulli-model.fig}
\end{figure}
%
The model treats the observed binary data, \code{y[1],...,y[N]}, as
independent and identically distributed, with success probability \code{theta}.  
A \code{beta(1,1)} (i.e., uniform) prior is placed on \code{theta} , although there is no
special behavior for conjugate priors in \proglang{Stan}.  This model
is available in the \proglang{Stan} source distribution in
\code{src/models/basic\_estimators/bernoulli.stan}.


\subsection{Data format}

Data for running \proglang{Stan} from the command line can be included
in \proglang{R} dump format.%
%
\footnote{A JSON interface for structured data input and output is
  currently under development.}
%
For example, 10 observations for the
model in Figure~\ref{bernoulli-model.fig} could be encoded as
%
\begin{Code}
N <- 10
y <- c(0,1,0,0,0,0,0,0,0,1)
\end{Code}
%
This defines the contents of two variables, an integer \code{N} and a
10-element integer array \code{y}.  In \pkg{RStan} and \pkg{PyStan},
data can also be passed directly through memory without the need to
read or write to a file.  This data file is provided with the
\proglang{Stan} distrbution in file
\code{src/models/basic\_estimators/bernoulli.R.stan}.

\subsection{Compling the model}

After a \proglang{C++} compiler and \code{make} are installed,%
%
\footnote{Appropriate versions are built into Linux. The \pkg{RTools}
  package suffices for Windows; it is available from
  \url{http://cran.r-project.org/bin/windows/Rtools/}.  The
  \pkg{Xcode} package contains everything needed for the Mac; see
  \url{https://developer.apple.com/xcode/} for more information.}
%
the Bernoulli model in Figure~\ref{bernoulli-model.fig} can be
translated to \proglang{C++} and compiled with a single command.
First, the directory must be changed to \code{$stan}, which we use as
a shorthand for the directory in which \proglang{Stan} was unpacked.%
%
\footnote{Before the first model is built, \code{make} must build the
  model translator (target \code{bin/stanc}) and posterior summary tool
  (target \code{bin/print}), along with an optimized version of the
  \proglang{C++} library (target \code{bin/libstan.a}).  Please be patient
  and consider \code{make} option \code{-j2} or \code{-j4} (or higher)
  to run in the specified number of processes if two or four (or more)
  computational cores are available.}
%
\begin{CodeChunk}
\begin{CodeInput}
> cd $stan
> make src/models/basic_estimators/bernoulli 
\end{CodeInput}
\end{CodeChunk}
%
This produces an executable file \code{bernoulli}
(\code{bernoulli.exe} on Windows) on the same path as the model.
Forward slashes can be used with \code{make} on Windows.

\subsection{Running the model}

\subsubsection{Command to run the model}

The executable can be run with default options by specifying a path to
the data file.  The first command in the following example changes the
current directory to that containing the model, which is where the
data resides and where the executable is built.  From there, the path
to the data is just the file name \code{bernoulli.data.R}.
%
\begin{CodeChunk}
{\small
\begin{CodeInput}
> cd $stan/src/models/basic_estimators
./bernoulli --data=bernoulli.data.R --samples=samp1.csv --chain_id=1 --seed=7386
\end{CodeInput}
}
\end{CodeChunk}
%
For Windows, the \code{./} before the command should be removed.  This
invocation specifies the executable program, the data file, the file
to which the samples are written, the identity of the chain, and a
seed for the random number generator.

\subsubsection{Terminal output}

The output is as follows, starting with a summary of the command-line
options used, including defaults;  these are also written into the 
samples file as comments.
%
\begin{Code}
STAN SAMPLING COMMAND
data = bernoulli.data.R
init = random initialization
init tries = 1
samples = samp1.csv
append_samples = 0
save_warmup = 0
seed = 7386 (user specified)
chain_id = 1 (user specified)
iter = 2000
warmup = 1000
thin = 1
equal_step_sizes = 0
nondiag_mass = 0
leapfrog_steps = -1
max_treedepth = 10
epsilon = -1
epsilon_pm = 0
delta = 0.5
gamma = 0.05
Iteration:    1 / 2000 [  0%]  (Warmup)
Iteration:   10 / 2000 [  0%]  (Warmup)
...
Iteration: 1990 / 2000 [ 99%]  (Sampling)
Iteration: 2000 / 2000 [100%]  (Sampling)

# Elapsed Time: 0.011269 seconds (Warm-up)
#               0.014105 seconds (Sampling)
#               0.025374 seconds (Total)
\end{Code}
%
A description of all parameters is available with the \code{{-}{-}help}
option; they provide control over input, output, and the configuration
of the sampler.

\subsubsection{Samples file output}

The output CSV file starts with a summary of the parameters for the
run.  
%
\begin{Code}
# Samples Generated by Stan
#
# stan_version_major=1
# stan_version_minor=3
# stan_version_patch=0
# model=bernoulli_model
# data=bernoulli.data.R
# init=random initialization
# append_samples=0
# save_warmup=0
# seed=7386
# chain_id=1
# iter=2000
# warmup=1000
# thin=1
# nondiag_mass=0
# equal_step_sizes=0
# leapfrog_steps=-1
# max_treedepth=10
# epsilon=-1
# epsilon_pm=0
# delta=0.5
# gamma=0.05
# algorithm=NUTS with a diagonal Euclidean metric
\end{Code}
%
It continues with a header indicating the columns of the output.
%
\begin{Code}
lp__,accept_stat__,stepsize__,treedepth__,theta
\end{Code}
%
The values for \code{lp\_\_} indicate the log probability (up to an
additive constant).  The column headed by \code{accept\_stat\_\_}
provides the Metropolis acceptance statistic for each iteration.%
%
\footnote{Acceptance is the usual notion for a Metropolis-Hastings
  sampling such as HMC.  For NUTS, the acceptance statistic is 
  defined as the average acceptance probabilities of all possible
  samples in the proposed tree.}
%
The column \code{stepsize\_\_} indicates the step size
(i.e., time interval) of the simulated trajectory, while the column 
\code{treedepth\_\_} gives the tree depth for NUTS, defined as the 
log base 2 of the total number of steps in the trajectory.
The rest of the header will be the names of parameters; in this
example, \code{theta} is the only parameter.

Next, the results of adaptation are printed.
%
\begin{Code}
# Adaptation terminated
# Step size = 2.10898
# Diagonal elements of inverse mass matrix:
# 0.465584
\end{Code}
%
By default, the execution above uses a diagonal mass matrix, which
with one parameter is a single element.

The rest of the file contains samples, one per line, matching the
header; here the parameter \code{theta} is the final value printed on
each line, and each line corresponds to a sample.  The warmup samples
are not included by default, but may be included with the appropriate
command-line invocation of the executable. The file ends with comments reporting the elapsed time.
%
\begin{Code}
-7.04574,0.189209,2.10898,0,0.353589
-7.04574,0.00162111,2.10898,0,0.353589
-6.77448,1,2.10898,0,0.222005
...
-7.89094,0.346733,2.10898,0,0.46154
-6.93412,1,2.10898,0,0.330879
-6.93412,0.115548,2.10898,0,0.330879

# Elapsed Time: 0.011269 seconds (Warm-up)
#               0.014105 seconds (Sampling)
#               0.025374 seconds (Total)
\end{Code}
%

\subsection{Output analysis}

Before performing output analysis, we recommend generating multiple
independent chains in order to more effectively monitor convergence.
Three more chains of samples can be created as follows.
%
\begin{CodeChunk}
{\small
\begin{CodeInput}
> ./bernoulli --data=bernoulli.data.R --samples=samp2.csv --chain_id=2 --seed=7386
> ./bernoulli --data=bernoulli.data.R --samples=samp3.csv --chain_id=3 --seed=7386
> ./bernoulli --data=bernoulli.data.R --samples=samp4.csv --chain_id=4 --seed=7386
\end{CodeInput}
}
\end{CodeChunk}
%
The chains can be safely run in parallel under different processes;
details of parallel execution depend on the operating system and the shell or
terminal program. Note that, although the same seed is used for each chain, the
random numbers will in fact be independent as the chain identifier
is used to skip the pseudorandom number generator ahead.  See
Section~\ref{rng.section} for more information.

\proglang{Stan} supplies a command-line program \code{bin/print} to
summarize the output of one or more MCMC chains.  Given a directory
containing output from sampling, it is invoked as
%
\begin{CodeChunk}
\begin{CodeInput}
> ls samp*.csv
\end{CodeInput}
\begin{CodeOutput}
samp1.csv	samp2.csv	samp3.csv	samp4.csv
\end{CodeOutput}
\begin{CodeInput}
> ../../../bin/print samp*.csv
\end{CodeInput}

\end{CodeChunk}
%
The output is shown in Figure~\ref{print-output.fig}.%
%
\footnote{Aligning columns when printing rows of varying scales
  presents a challenge.  For each column, the program calculates the
  the maximum number of digits required to print an entry in that
  column with the specified precision. For example, a precision of 2
  for the number -0.000012 requires nine characters (\code{-0.000012})
  to print without scientific notation versus seven digits with
  (\code{-1.2e-5}).  If the discrepancy is above a fixed threshold,
  scientific notation is used.  Compare the results in the \code{mean}
  column versus the \code{sd} column.}
%

\begin{figure}
\small
\begin{Code}
Inference for Stan model: bernoulli_model
4 chains: each with iter=(1000,1000,1000,1000); warmup=(0,0,0,0); thin=(1,1,1,1); 
4000 iterations saved.

Warmup took (0.012, 0.010, 0.011, 0.011) seconds, 0.046 seconds total
Sampling took (0.016, 0.014, 0.014, 0.015) seconds, 0.060 seconds total

                  mean   se   sd     2.5%      50%    97.5%   n_eff n_eff/s    Rhat
lp__          -7.3e+00  0.0  0.8 -9.3e+00 -7.0e+00 -6.7e+00 1.1e+03 1.9e+04 1.0e+00
accept_stat__  5.3e-01  0.1  0.4  1.1e-06  5.2e-01  1.0e+00 3.7e+03 6.1e+04 1.0e+00
stepsize__     2.1e+00  0.1  0.2  2.0e+00  2.1e+00  2.1e+00 2.0e+00 3.3e+01 1.3e+13
treedepth__    3.6e-02  0.0  0.2  0.0e+00  0.0e+00  1.0e+00 2.9e+03 4.9e+04 1.0e+00
theta          2.5e-01  0.0  0.1  6.0e-02  2.4e-01  4.9e-01 2.1e+03 3.5e+04 1.0e+00

Samples were drawn using NUTS with a diagonal Euclidean metric.
For each parameter, n_eff is a crude measure of effective sample size,
and Rhat is the potential scale reduction factor on split chains (at 
convergence, Rhat=1).
\end{Code}
\caption{Output of \code{bin/print} for the Bernoulli estimation model in
  Figure~\ref{bernoulli-model.fig}.}\label{print-output.fig}
\end{figure}
%
Each row of the output summarizes a different value whose name is
provided in the first column.  The analysis includes the posterior
mean (\code{mean}) and standard deviation (\code{sd}).  The median
(\code{50\%}) and 95\% posterior interval (\code{2.5\%}, \code{97.5\%})
are also displayed; the \code{bin/print} command also prints the 50\%
interval boundaries, but these are ellided for space in the above
output.

The estimated potential scale reduction statistic $\hat{R}$
(\code{Rhat}) value of 1.0 for the parameter of interest \code{theta}
is consistent with convergence.  \proglang{Stan} uses a more
conservative version of $\hat{R}$ than is usual in packages such as
\pkg{Coda} \citep{PlummerEtAl:2006}, first splitting each chain in half to diagnose
montonically increasing or decreasing samples; \citep{GelmanEtAl:2013}
and \citep{Stan:2013} provide detailed definitions.

The estimated number of effective samples per parameter
(\code{n\_eff}) is more than 2000, far more than we actually need for
inference.  As with $\hat{R}$, the number of effective samples is
estimated conservatively using both cross-chain and within-chain
estimates; \citep{GelmanEtAl:2013} and the reference manual provide
details.

The MCMC standard error (\code{se}) is estimated by the posterior
standard deviation estimate divided by the square root of the
estimated number of effective samples (\code{sd / sqrt(n\_eff)}).  In
this example, the MCMC standard error is small relative to the
posterior standard deviation, another sign that a sufficient number
of samples have been drawn.


\section{Models}

In the rest of this paper, we will concentrate on the modeling
language and how compiled models are executed.  These details are the
same whether a \proglang{Stan} model is being used by a built-in or
externally supplied sampler or optimizer.

\subsection{Example: hierarchical model, with inference}

\cite[Section 5.1]{GelmanEtAl:2013} define a hierarchical model of the
incidence of tumors in rats in control groups across trials; a very
similiar model is defined for mortality rates in pediatric surgeries
across hospitals in \citep[Examples, Volume
1]{LunnEtAl:2000,LunnEtAl:2009}.  
%  
\begin{figure}
\begin{Code}
data {
  int<lower=0> J;                         // number of items
  int<lower=0> y[J];                      // number of successes for j
  int<lower=0> n[J];                      // number of trials for j
}
parameters {
  real<lower=0,upper=1> theta[J];        // chance of success for j
  real<lower=0,upper=1> lambda;          // prior mean chance of success
  real<lower=0.1> kappa;                 // prior count
}
transformed parameters {
  real<lower=0> alpha;                   // prior success count
  real<lower=0> beta;                    // prior failure count
  alpha <- lambda * kappa;
  beta <- (1 - lambda) * kappa;
}
model {
  lambda ~ uniform(0,1);                 // hyperprior
  kappa ~ pareto(0.1,1.5);               // hyperprior 
  theta ~ beta(alpha,beta);              // prior
  y ~ binomial(n,theta);                 // likelihood
}
generated quantities {
  real<lower=0,upper=1> avg;             // avg success
  int<lower=0,upper=1> above_avg[J];     // true if j is above avg
  int<lower=1,upper=J> rnk[J];           // rank of j
  int<lower=0,upper=1> highest[J];          // true if j is highest rank
  avg <- mean(theta);
  for (j in 1:J)
    above_avg[j] <- (theta[j] > avg);
  for (j in 1:J) {
    rnk[j] <- rank(theta,j) + 1;
    highest[j] <- rnk[j] == 1;
  }
}
\end{Code}
  \caption{Hierarchical binomial model with posterior inferences,
    coded in \proglang{Stan}.}\label{hier-binom.fig}
\end{figure}
%
A \proglang{Stan} implementation is provided in
Figure~\ref{hier-binom.fig}.  In the rest of this section, we will
walk through what the meaning of the various blocks are for the
execution of the model.

\subsection{Data block}

A \proglang{Stan} program starts with an (optional) data block, which
declares the data required to fit the model.  In the model in
Figure~\ref{hier-binom.fig}, the data block declares an integer
variable \code{J} for the number of groups in the hierarchical model.
The arrays \code{y} and \code{n} have size \code{J}, with \code{y[j]}
being the number of positive outcomes in \code{n[j]} trials.  

All of these variables are declared with a lower-bound constraint
restricting their values to be greater than or equal to zero.
\proglang{Stan}'s constraint language is not strong enough to restrict
each \code{y[j]} to be less than or equal to \code{n[j]}.

The data for a \proglang{Stan} model is read in once as the
\proglang{C++} object representing the model is constructed.  After
the data is read in, the constraints are validated.  Failure to
validate data will terminate the program and print an informative
error message.

\subsection{Transformed data block}

The model in Figure~\ref{hier-binom.fig} does not have a transformed
data block.  A transformed data block may be used to define new
variables that can be computed based on the data.  For example,
standardized versions of data can be defined in a transformed
data block or Bernoulli trials can be summed to model as binomial.
Any constant data can also be defined in the transformed data block.

The transformed data block starts with a sequence of variable
declarations and continues with a sequence of statements defining the
variables.  For example, the following transformed data block declares
a vector \code{x\_std}, then defines it to be the standardization of \code{x}.
%
\begin{Code}
transformed data {
  vector[N] x_std;
  x_std <- (x - mean(x)) / sd(x);
}
\end{Code}

The transformed data block is executed during construction, after the
data is read in.  Any data variables declared in the data block may be
used in the variable declarations or statements.  Transformed data
variables may be used after they are declared, although care must be
taken to ensure they are defined before they are used.  Any
constraints declared on transformed data variables are validated after
all of the statements are executed, with execution terminating with an
informative error message at the first variable with an invalid value.

\subsection{Parameter block}

The parameter block in the program in Figure~\ref{hier-binom.fig}
defines three parameters.  The parameter \code{theta[j]} represents
the probability of success in group \code{j}.  The prior on each
\code{theta[j]} is parameterized by a prior mean chance of success
\code{lambda} and prior count \code{kappa}.  Both \code{theta[j]} and
\code{lambda} are constrained to fall between zero and one, whereas
\code{lambda} is constrained to be greater than or equal to 0.1 to
match the support of the Pareto hyperprior it receives in the model block.

The parameter block is executed every time the log probability is
evaluated.  This may be multiple times per iteration of a sampling or
optimization algorithm.  Furthermore, different samplers and
optimizers use different instantiations of the log probability
function depending on the form and order of the derivative information
they require; see Section~\ref{inference-engines.section} for
details. 

\subsubsection{Implicit change of variables to unconstrained space}

The probability distribution defined by a \proglang{Stan} program is
intended to have unconstrained support (i.e., no points of zero
probabilty), which greatly simplifies the task of
writing samplers or optimizers.  To achieve unbounded support,
variables declared with constrained support are transformed 
to an unconstrained space.  For
instance, variables declared on $[0,1]$ are log-odds transformed and
non-negative variables declared to fall in $[0,\infty)$ are log
transformed.  More complex transforms are required for simplexes (a
reverse stick-breaking transform) and covariance matrices (Cholesky
factorization).  The dimensionality of the resulting probabilty
function may change as a result of the transform. For example, a $K
\times K$ covariance matrix requires only ${K \choose 2} + K$
unconstrained parameters, and a $K$-simplex requires only $K-1$
unconstrained parameters.

The unconstrained parameters over which the model is defined are
inverse transformed back to their constrained forms before executing
the model code.  To account for the change of variables, the log
absolute Jacobian determinant of the inverse transform is added to the
overall log probabilty function.%
%
\footnote{For optimization, the Jacobian adjustment may be optionally
  suppressed to guarantee the optimizer finds the maximum of the log
  probability function on the constrained parameters.}
%
The gradients of the log probabilty function exposed include the
Jacobian term.  

There is no validation required for the parameter block because the
variable transforms are guaranteed to produce values that satisfy the
declared constraints.


\subsection{Transformed parameters block}

The transformed parameters block allows users to define transforms of
parameters within a model.  Following the model in
\citep{GelmanEtAl:2013}, the example in Figure~\ref{hier-binom.fig}
uses the transformed parameter block to define transformed parameters
\code{alpha} and \code{beta} for the prior success and failure counts
to use in the beta prior for \code{theta}.  

Following the same convention as the transformed data block, the
(optional) transformed parameter block begins with declarations of the
transformed parameters, followed by a sequence of statements defining
them.  Variables from previous blocks as well as the transformed
parameters block may be used.  In the example, the prior success and
failure counts \code{alpha} and \code{beta} are defined in terms of
the prior mean \code{lambda} and total prior count \code{kappa}.

The transformed parameter block is executed after the parameter
block.  Constraints are validated after all of the statements defining the
transformed parameters have executed.  Failure to validate a
constraint results in an exception being thrown, which halts the
execution of the log probability function.  The log probability
function can be defined to return negative infinity or a special
not-a-number value. 

If transformed parameters are used on the left-hand side of a sampling
statement, it is up to the user to add the appropriate log Jacobian
adjustment to the log probability accumulator.  For instance, a
lognormal variate could be generated as follows without the built-in
\code{lognormal} density function  using the normal density as
%
\begin{Code}
parameters {
  real<lower=0> u;
  ...
transformed parameters {
  real v;
  v <- log(u);
  lp__ <- lp__ + u;   // log Jacobian adjustment
}
model {
  v ~ normal(0,1);
}
\end{Code}
%
The transorm is $f(u) = \log u$, the inverse transform is $f^{-1}(v) =
\exp v$, so the absolute log Jacobian is $|\frac{d}{dv} \exp v| =
\exp v = u$.  Whenever a transformed parameter is used on the left
side of a sampling statement, a warning is printed to remind the user
of the need for a Jacobian adjustment for the change of variables.

Values of transformed parameters are saved in the output along
with the parameters.  As an alternative, local variables can be used
to define temporary values that do not need to be saved.  

\subsection{Model block}

The model block defines the log probability function.  The example in
Figure~\ref{hier-binom.fig} has a simple model containing four
sampling statements.  The hyperprior on the prior mean \code{lambda}
is uniform, and the hyperprior on the prior count \code{kappa} is a
Pareto distribution with lower-bound of support at 0.1 and shape 1.5,
leading to a probability of $\kappa > 0.1$ proportional to
$\kappa^{-5/2}$.  Note that the hierarchical prior on \code{theta} is
vectorized: each elements of \code{theta} is drawn independently from
a beta distribution with prior success count \code{alpha} and prior
failure count \code{beta}.  Both \code{alpha} and \code{beta} are
transformed parameters, but because they are only used on the
right-hand side of a sampling statement do not require a Jacobian
adjustment of their own.  The likelihood function is also vectorized,
with the effect that each success count \code{y[i]} is drawn from a
binomial distribution with number of trials \code{n[i]} and chance of
success \code{theta[i]}.  In vectorized sampling statements, single
values may be repeated as many times as necessary.  

The model block is executed after the transformed parameters block
every time the log probability function is evaluated.  

\subsection{Generated quantities block}

The (optional) generated quantities allows values that depend on
parameters and data, but do not affect estimation, to be defined
efficiently.  The generated quantities block is called only once per
sample, not once per log probability function evaluation.  It may be
used to calculate predictive inferences as well as to carry out
forward simulation for predictive posterior checks.

The \proglang{BUGS} surgical example explored the ranking of
institutions in terms of surgical mortality.  This is coded in the
example in Figure~\ref{hier-binom.fig} using the generated quantities
block.  The generated quantity variable \code{rnk[j]} will hold the
rank of institution \code{j} from 1 to \code{J} in terms of mortality
rate \code{theta[j]}.  The ranks are extracted using the \code{rank}
function. The posterior summary will print average rank and deviation,
but the values can also be extracted in order to plot posterior rank
histograms by institution as done in \citep[Examples, Volume
1]{LunnEtAl:2000}.

Posterior comparisons can be carried out directly or using rankings.
For instance, the model in Figure~\ref{hier-binom.fig} sets
\code{highest[j]} to 1 if hospital \code{j} has the highest estimated
mortality rate.  Applying a hierarchical model then considering
posterior inference appropriately adjusts for mulitple comparisons and
allows; see \citep{GelmanEtAl:2012, Efron:2010} for discussion.

As a second illustration, the generated quantities block calculate the
(posterior) probability that a given institution is above-average in
terms of mortality rate.  This is done for each institution \code{j}
with the usual plug-in estimate of \code{theta[j] > mean(theta)},
which returns a binary (0 or 1) value.  The posterior mean of
\code{above\_avg[j]} calculates the posterior probability
$\mbox{Pr}[\theta_j > \bar{\theta}|y,n]$ according to the model.

\subsection{Initialization}

\proglang{Stan}'s samplers and optimizers all start from either random
or user-supplied values for each parameter.  User
supplied initial values are validated and transformed to
the underlying unconstrained space; if a parameter value does not satisfy its
declared constraints, the program exits and an informative error
message is printed.  If random initialization is specified, the
built-in pseudorandom number generator is called once per
unconstrained variable dimension.  The default initialization is to
randomly generate values uniformly on $[-2,2]$, which supplies fairly
diffuse starting points when transformed back to the constrained
scale.

\subsection{Variable definition and block execution summary}

A table summarizing the point at which variables are read, written,
or defined is provided in Figure~\ref{block-actions.fig}. 
%
\begin{figure}
\begin{center}
\begin{tabular}{l|c|l|l}
{\it Block} & {\it Statements?} & {\it Action} & {\it Evaluated}
\\\hline\hline
\code{user initialization} & n/a & transform & chain
\\[3pt]
\code{random initialization} & n/a & randomize & chain 
\\\hline\hline
\code{data} & no & read & chain  
\\
\code{transformed data} & yes & evaluate & chain  
\\ \hline
\code{parameters} & no & inv.\ transform, Jacobian & leapfrog  \\
& & inv.\ transform, write & sample 
\\[3pt]
\code{transformed parameters} & yes & evaluate & leapfrog \\
& & write & sample 
\\\hline
\code{model} & yes & evaluate & leapfrog
\\\hline
\code{generated quantities} & yes & evaluate & sample \\
& & write & sample
\end{tabular}
\end{center}
\caption{Each \proglang{Stan} program block admits certain actions that are evaluated
  at specific times during sampling.  For example, the parameter initialization in
  the \code{data} block requires a read operation once per chain.}
\label{block-actions.fig}
\end{figure}
%
This table is defined assuming HMC or NUTS samplers, which require a
log probability and gradient calculation for one or more leapfrog
steps per iteration.  

%
\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Variable Categorization} & {\it Declaration Block}
\\ \hline\hline
% constants & \code{transformed data}
% \\ \hline
unmodeled data & \code{data}, \code{transformed data}
\\ 
modeled data & \code{data}, \code{transformed data}
\\ \hline
missing data & \code{parameters}, \code{transformed parameters}
\\
modeled parameters & \code{parameters}, \code{transformed parameters}
\\
unmodeled parameters & \code{data}, \code{transformed data}
\\[2pt] \hline
generated quantities & \code{transformed data}, \code{transformed parameters}, 
\\ 
& \code{generated quantities}
\\ \hline\hline
loop indices & any loop statement
\\ \hline
local variables & any statement block
\\ 
\end{tabular}
\end{center}
\caption{Variables of each categorization must be
  declared in specific blocks.  Data
  may also be expressed using numeric
  literals.}\label{variable-kinds.fig}
\end{figure}
%
\cite[p.~366]{GelmanHill:2007} provide a taxonomy of the kinds of
variables used in Bayesian models.  Figure~\ref{variable-kinds.fig}
contains Gelman and Hill's taxonomy aligned with the corresponding
locations of declarations and definitions in \proglang{Stan}.
Unmodeled data variables includes size constants and regression
predictors.  Modeled data variables include known outcomes or
measurements.  A data variable or constant literal is modeled in a
\proglang{Stan} program if it (or a variable that depends on it)
occurs on the left-hand side of a sampling statement.  Unmodeled
parameters are provided as data, and are either known or fixed to some
value for convenience, a typical use being the parameters of a weakly
informative prior for a parameter.

A modeled parameter is given a distribution by the model (usually
dependent on data and correlated with other parameters).  This can be
done either by placing it (or a variable that depends on it) on the
left-hand side of a sampling statement or by declaring the variable
with a constraint (see Section~\ref{implicit-prior.section}).

Any variable that occurs in a (transformed) data block can also be
provided instead as a constant.  So a user may decide to write \code{y
  ~ normal(0,1)} or to declare \code{mu} and \code{sigma} as data, and
write \code{y ~ normal(mu,sigma)}.  The latter choice allows the
parameters to be changed without recompiling the model, but requires
them to be specified as part of the input data.

Missing data is a new variable type included here.  In order to
perform inference on missing data, it must be declared as a parameter
and modeled; see \citep{GelmanEtAl:2013} for a discussion of
statistical models of missing data.  Unlike \proglang{BUGS}, data may
not contain a mixture of missing (i.e., \code{NA}) values and numeric
values.  \proglang{Stan} uses the built-in \proglang{C++}
floating-point (and integer) arithmetic, for which there is no
equivalent of \code{NA}.  See \citep{Stan:2013} for strategies for
coding missing data problems in \proglang{Stan}.

Generated quantities include simple transforms of data required for
printing.  For example, the variable of interest might be a standard
deviation, resulting from transforming a precision parameter $\tau$ to
a scale parameter $\sigma = \tau^{-1/2}$.  A non-linearly transformed
variable will have different effective sample sizes and $\hat{R}$
statistics than the variable it is derived from, so it is convenient
to define variables of interest in the generated quantities block to
calculate these statistics automatically.  As seen in the example in
Figure~\ref{hier-binom.fig}, generated quantities may also be used for
event probability estimates.

The generated quantities block may also be used for forward
simulations, generating values to make predictions or to perform
posterior predictive checks; see \citep{GelmanEtAl:2013} for more
information.  The generated quantities block is the only location in
\proglang{Stan} in which random-number generators may be applied
explicitly (they are implicit in parameters).

Calculations in the generated quantities block do not impact the
estimates of the parameters.  In this way, it can be used like the cut
feature of \proglang{BUGS}.  Nevertheless, we recommend full Bayesian
inference in general, and do not use it for cut-like behavior unless
it does not effect inference (e.g., for generated predictions for
unseen data in many models).

The list is completed with two types of local variables, loop indices
and traditional local variables.  Unlike \proglang{BUGS},
\proglang{Stan} allows local variables to be declared and assigned.
For example, it is possible to compute the sum of the squares of
entries in an array or vector \code{y} as follows.
%
\begin{Code}
{ 
  real sum_of_squares;
  sum <- 0;
  for (n in 1:N)
    sum <- sum + y[n] * y[n];
}  
\end{Code}
%
Local variables may not be declared with constraints, because there is
no location at which it makes sense to test that they satisfy the
constraints. 


\subsection{Implicit Uniform Priors}\label{implicit-prior.section}

The default distribution for a variable is uniform over its support.
For instance, a variable declared with a lower bound of 0 and an upper
bound of 1 implicitly receives a $\mbox{\sf Uniform}(0,1)$
distribution.  These implicit uniform priors are improper if the
variable has unbounded support.  For instance, the uniform
distribution over simplexes and correlation matrices is proper, but
the uniform distribution over ordered vectors or covariance matrices
is not. 

\subsection{The ``Matt trick'' for hierarchical models}

Consider the following hierarchical logistic regression model fragment.
%
\begin{Code}
data {
  int<lower=1,upper=K> group[N];  // group of data item n
}  
parameters {
  ...
  vector mu;             // mean coeff
  real<lower=0> sigma;   // coeff scale
  vector[K] beta;        // coeff for group k
}
model {
  beta ~ normal(mu,sigma);
  for (n in 1:N)
    y[n] ~ bernoulli_logit(beta[group[n]] * x[n]);
  ...
}
\end{Code}
%
With the direct parameterization, a change in \code{sigma} is
amplified by the lower-level parameters and introduces a large
change in density across the posterior.  Unfortunately the 
expected density variation of an HMC transition is limited%
%
\footnote{The limited density variation is a ultimately consequence
of a constant metric.  An additional benefit of RMHMC are transitions
that can cover much larger variations in density, making it uniquely
suited to these models.}
%
and many transitions are required to explore the full posterior.
The resulting sampler devolves into a random walk with high autocorrelations. 

The following reparameterization employs the ``Matt trick'',
%
\footnote{The reparameterization is named after Matt Hoffman, who
  introduced it during modeling discussions in the \proglang{Stan}
  group meetings.  A similar reparameterization by \citep{Choo:2000}
  is discussed in \citep[Section~5.4.5]{Neal:2011}.}
%
\begin{Code}
parameters {
  vector[K] beta_raw;    
}
transformed parameters {
  vector[K] beta;
  beta <- mu + sigma * beta_raw;
}
model {
  beta ~ normal(0,1);
}
\end{Code}
%
This reparameterization removes the particular correlations amongst the
hierarchical variables that would otherwise limit the effectiveness
of the samplers.

\section{Data types}

Expressions in \proglang{Stan}, including variables are statically
typed.  This means their type is declared at compile time as part of
the model, and does not change throughout the execution of the
program.  This is the same behavior as is found in compiled
programming languages such as \proglang{C(++)}, \proglang{Fortran},
and \proglang{Java}, but is unlike the behavior of interpreted
languages such as \proglang{BUGS}, \proglang{R}, and
\proglang{Python}.  Statically typing the variables (as well as
declaring them in appropriate blocks based on usage) makes
\proglang{Stan} programs easier to read and easier to debug by making
explicit the modeling decisions and expression types.

\subsection{Primitive Types}

The primitive types of \proglang{Stan} are \code{real} and \code{int},
which are used to represent continuous and integral values.  These
values are represented directly in \proglang{C++}.  Integer
expressions can be used anywhere a real value is required, but not
{\it vice-versa}.

\subsection{Vector and Matrix Types}

\proglang{Stan} supports vectors, row vectors, and matrices with the
usual access operations.  Indexing for vector, matrix, and array types
starts from one.  

Vectors are declared with their sizes and matrices with their number
of rows and columns.

All vector and matrix types contain real values and may not be
declared to contain integers.  Collections of integers are represented
using arrays.

\subsection{Array Types}

An array may have entries of any other type.  For example, arrays of
integers and reals are allowed, as are arrays of vectors or arrays of
matrices.  

Higher-dimensional arrays are intrinsically arrays of arrays.  An
entry two-dimensional array \code{y} may be accessed as \code{y[1,2]}
or \code{y[1][2]}, with the expression \code{y[1]} denoting an array.%
%
\footnote{ Arrays are stored in row-major order and matrices in
  column-major order.}

\subsection{Constrained Variable Types}

Variables may be declared with constraints.  The constraints have
different effects depending on the block in which the variable is
declared.

Integer and real types may be provided with lower bounds, upper
bounds, or both.  This includes the types used in arrays, and the real
types used in vectors and matrices.

Vector types may be constrained to be unit simplexes (all entries
non-negative and summing to 1), unit length (sum of squares is 1), or
ordered (entries are in ascending order), positive ordered (entries in
ascending order, all non-negative), using the types \code{simplex[K]},
\code{unit_vector[K]}, \code{ordered[K]}, or
\code{positive_ordered[K]}, where \code{K} is the size of the vector.

Matrices may be constrained to be covariance matrices (symmetric,
positive definite) or correlation matrices (symmetric, positive
definite, unit diagonal), using the types \code{cov_matrix[K]} and
\code{corr_matrix[K]}.

\section{Expressions and type inference}

The syntax of \code{Stan} is defined in terms of expressions and
statements.  Expressions denote values of a particular type.  
Statements represent operations such as assignment and sampling as
well as control structures such as for loops and conditionals.

\subsection{Expressions}

\proglang{Stan} provides the usual kinds of expressions found in
programming languages.  This includes variables, literals denoting
integers, real values or strings, binary and unary operators over
expressions, and function application.  

\subsubsection{Type inference}

The type of each variable is declared statically and cannot change.  

The type of a numeric literal is determined by whether or not it
contains a period or scientific notation; for example, \code{20} has
type \code{int} whereas \code{20.0} and \code{2e+1} have type
\code{real}.

The type of applying an operator or a function to one or more
expressions is determined by the available signatures for the
function.  For example, the multiplication operator (\code{*}) has a
signature that maps two \code{int} arguments to an \code{int} and two
\code{real} arguments to a \code{real} result.  Another signature for
the same operator maps a \code{row\_vector} and a \code{vector} to a
\code{real} result.

\subsubsection{Type promotion}

If necessary, an integer type will be promoted to a \code{real} value.
For example, multiplying an \code{int} by a \code{real} produces a
\code{real} result by promoting the \code{int} argument to a
\code{real}.  \proglang{Stan} also attempts to treat arrays as
covariant, meaning that arrays of of \code{int} values can often be
used where arrays of \code{real} values are specified.

\section{Statements}

\subsection{Assignment and sampling}

\proglang{Stan} supports the same two basic statements as
\proglang{BUGS}, assignment and sampling, examples of which were
introduced earlier.  In \proglang{BUGS}, these two kinds of statment
define a directed acyclic graphical model; in \proglang{Stan}, they
define a log probability function. 

\subsubsection{Log probability accumulator}

There is an implicitly defined variable \code{lp\_\_} (available in
the transformed parameters and model blocks) denoting the log
probability that will be returned by the log probability function.  A
sampling statement is nothing more than shorthand for incrementing the
log probability accumulator \code{lp\_\_}.  For example, if \code{beta}
is a parameter of type \code{real}, the sampling statement
%
\begin{Code}
beta ~ normal(0,1);
\end{Code}
%
has the exact same effect as the assignment statement
%
\begin{Code}
lp__ <- lp__ + normal_log(beta,0,1);
\end{Code}

Because computation is only up to a proportion, this sampling
statement in turn has the same effect as the direct implementation in
terms of basic arithmetic,
%
\begin{Code}
lp__ <- lp__ - 0.5 * beta * beta;
\end{Code}
%
If \code{beta} is of type \code{vector}, replace \code{beta * beta}
with \code{beta' * beta}.  Distributions whose probability functions
are not built directly into \proglang{Stan} can be implemented
directly in this fashion.

\subsection{Sequences of statements and execution order}

\proglang{Stan} allows sequences of statements wherever statements may
occur. Unlike \proglang{BUGS}, in which statements define a directed
acyclic graph, in \proglang{Stan}, statements are executed
imperatively in the order in which they occur in a program.

\subsubsection{Blocks and variable scope}

Sequences of statements surrounded by curly braces (\code{\{} and
\code{\}}) form blocks.  Blocks may start with local variable
declarations.  The scope of a local variable (i.e., where it is
available to be used) is that of the block in which it is declared.

Other variables, such as those declared as data or parameters, may
only be assigned to in the block in which they are declared.  They may
be used in the block in which they are declared and may also be used
in any block after the block in which they are declared.


\subsection{Whitespace, semicolons, and comments}

Following the convention of \proglang{C++}, statements are separated
with semicolons in \proglang{Stan} so that the content of whitespace
(outside of comments) is irrelevant.  This is in contrast to
\proglang{BUGS} and \proglang{JAGS} in which carriage returns are
special and may indicate the end of a statement.

\proglang{Stan} supports the line comment style of \proglang{C++},
using two forward slashes (\code{//}) to comment out the rest of a
line; this is the one location where the content of whitespace
matters.  \proglang{Stan} also supports the line comment style of \proglang{R} and
\proglang{BUGS}, treating a pound sign (\code{\#}) as commenting out
everything until the end of the line.  \proglang{Stan} also supports
\proglang{C++}-style block comments, with everything between the
start-comment (\code{/*}) and end-comment (\code{*/}) markers being
ignored. 

The preferred style follows that of \proglang{C++}, with line comment
used for everything but multiline comments.

\subsection{Control structures}

\proglang{Stan} supports the same kind of explicitly bounded for loops
as found in \code{BUGS} and \code{R}.  Like \code{R}, but unlike
\proglang{BUGS}, \proglang{Stan} supports while loops and conditional
(if-then-else) statements.%
%
\footnote{\proglang{BUGS} omits these control structures because they
  would introduce ambiguities into the directed, acyclic graph defined
  by model.}
%
\proglang{Stan} provides the usual comparison operators and boolean
operators to help define conditionals and condition-controlled while
loops.  

\subsection{Print statements and debugging}

\proglang{Stan} provides print statements which take arbitrarily many
arguments consisting of expressions or string literals consisting of
sequences of characters surrounded by double quotes (\code{"}).
These statements may be used for debugging purposes to report on
intermediate states of variables or to indicate how far execution has
proceeded before an error.

\section{Function and distribution library}

In order to support the algorithmic differentiation required to
calculate gradients, Hessians, and higher-order derivatives in
\proglang{Stan}, we require \proglang{C++} functions that are 
templated separately on all of their arguments.  In order for these
functions to be efficient in computing both values and derivatives,
they need to be vectorized so that shared computations can be reused
and virtual function calls eliminated.

\subsection{Basic operators}

\proglang{Stan} supports all of the basic \proglang{C++} arithmetic
operators, boolean operators, comparison operators  In addition, it
extends the arithmetic operators to matrices and includes pointwise
matrix operators.%
%
\footnote{This is in contrast to \proglang{R} and \proglang{BUGS}, who
  treat the basic multiplication and division operators pointwise and
  use special symbols for matrix operations.}
%
The full set of operators is listed in Figure~\ref{operators.fig}.

\subsection{Special functions}

\proglang{Stan} provides an especially rich set of special functions.
This includes all of the \proglang{C++} math library functions, as
well as numberous more specialized functions such as Bessel functions,
gamma and digamma functions, and generalized linear model link
functions and their inverses.  There are also many compound functions,
such as \code{log1m(x)}, which is more stable arithmetically for
values of x near 0 than \code{log(1 - x)}.  \proglang{Stan}'s special
functions are listed in Figures~\ref{special-functions.fig} and
\ref{special-functions-cont.fig}.


\subsection{Matrix and linear algebra functions}

Following the usual convention in mathematics, matrix and array
indexing uses the usual square brackets (\code{[ ]}) operator, and
begins from 1.  \proglang{R}, \proglang{MATLAB}, and \proglang{BUGS},
but not of \proglang{C++}, \proglang{Python}, or \proglang{Java}.
If \code{Sigma} is a matrix, then \code{Sigma[n]} is the \code{n}-th
row of \code{Sigma}.

Various reductions are provided for arrays and matrices, such as sums,
means, standard deviations and norms.  Replications are also available
to copy a value into ever cell of a matrix.  Slices of matrices and
vectors may be accessed by row, column, or general sub-block
operations.

Matrix operators use the types of their operands to determine the type
of the result.  For instance, multiplying a vector by a row vector
returns a matrix, whereas multiplying a row vector by a vector returns
a real.  A postfix apostrophe (\code{'}) is used for matrix and vector
transposition.  For example, if \code{y} and \code{mu} are vectors and
\code{Sigma} is a square matrix, all of the same dimensionality, then
\code{y - mu} is a vector, \code{(y - mu)'} is a row vector, \code{(y
  - mu)' * Sigma} is a row vector, and \code{(y - mu)' * Sigma * (y -
  mu)} will be a real value.  Matrix division is provided, which is
much more arithmetically stable than inversion, e.g., \code{(y - mu)'
  / Sigma} computes the same function as \code{(y - mu)' *
  inverse(Sigma)}.  \proglang{Stan} also supports elementwise
multiplication (\code{.*}) and division (\code{./}).

Linear algebra functions are provided for trace, left and right
division, Cholesky factorization, determinants and log determinants,
inverses, eigenvalues and eigenvectors, and singular value
decomposition.  All of these operations may be applied to matrices of
parameters or constants.  Various functions are specialized for speed,
such as quadratic products, diagonal specializations, multiply by self
transposed, e.g., the previous example could be written as
\code{quad_form(Sigma, y - mu)}.

The full set of matrix and linear-algebra functions is listed in
Figure~\ref{matrix-functions.fig}; operators, which also apply to
matrices and vectors, are listed in Figure~\ref{operators.fig}.

\subsection{Probability functions}

Stan supports a growing collection of built-in univariate and
multivariate probability density and mass functions.  These
probability functions share various features of their declarations and
behavior.

All probability functions are defined on the log scale to avoid
underflow.  They are all named with the suffix \code{\_log}, e.g.
\code{normal\_log()}, is the log-scale normal distribution density
function.

All probability functions check that their arguments are within the
appropriate constrained support and may be configured to throw
exceptions or return $-\infty$ or a special not-a-number value
(\code{NaN}) for out-of-domain arguments (the behavior of positive and
negative infinity and not-a-number values are built into
floating-point arithmetic).  For example,
\code{normal\_log(y,} \code{mu,} \code{sigma)} requires the scale parameter
\code{sigma} to be non-negative.

The list of probability functions is provided in Figures
\ref{prob-functions.fig}, \ref{prob-functions-cont.fig}, and
\ref{prob-functions-cont-2.fig}. 

\subsubsection{Up to a proportion calculations}

All probability functions support calculating results up to a constant
proportion, which becomes an additive constant on the log scale.
Constancy here refers to being a numeric literal such as \code{1}, a
constant function such as \code{pi()}, a variable declared as data or
transformed data, or an expression involving only other constants.
Non-constants include parameters, transformed parameters, local
variables declared in the transformed parameters or model statements,
as well as any expression involving a non-constant. 

Constant terms are dropped from probability function calculations at
the time the model is compiled, so there is no run-time overhead to
decide which expressions denote constants.%
%
\footnote{Both vectorization and dropping constant terms are
  implemented in \proglang{C++} through template metaprograms that
  infer traits of template arguments to the probability functions.
  Whether to drop constants is configurable through a boolean template
  parameter on the log probability and derivative functions generated
  in \proglang{C++} for a model.}
%
For example, calling \code{normal\_log(y,0,1)}, where \code{y} is a
parameter and \code{0} and \code{1} are constants, does not bother
computing the $\log \sigma$ term in the log density because $\sigma=1$
is a constant.  On the other hand, the $y - 0$ term is calculated and
is not a constant because $y$ is not a constant.

\subsubsection{Vectorization}

All of the univariate%
%
\footnote{We are in the process of vectorizing the multivariate
  probability functions, but they are not all available in
  \proglang{Stan} 2.0.  We are also in the process of vectorizing
  all of the special functions.}
%
probability functions in \proglang{Stan} are vectorized so that they
accept arrays or vectors of arguments.  For example, although the
basic signature of the probability function
\code{normal\_log(y,mu,sigma)} involves real \code{y}, \code{mu} and
\code{sigma}, it supports calls in which any any or all of \code{y},
\code{mu} and \code{sigma} contain more than one element. A typical
use case would be for linear regression, such as \code{normal\_log(y,X
  * beta,sigma)}, where \code{y} is a vector of observed data,
\code{X} is a predictor matrix, \code{beta} is a coefficient vector,
and \code{sigma} is a real value for the noise scale.

The advantage of using vectorization is twofold.  First, the models
are more concise and closer to mathematical notation.  Second, the
vectorized versions are much faster.  They reduce the number of times
expensive operations need to be evaluated and also reduce the number
of virtual function calls required in the compiled \proglang{C++}
executable for calculating gradients and higher-order derivatives.
In the example above, \code{normal\_log(y,X *
  beta,sigma)}, the logarithm of \code{sigma} need only be computed
once; if \code{y} is an $N$-vector, it also reduces the number of
virtual function calls from $N$ to 1.

\section{Built-in inference engines}\label{inference-engines.section}

\proglang{Stan} includes several Markov chain Monte Carlo (MCMC)
samplers and several optimizers.  Others may be straightforwardly
implemented within \proglang{Stan}'s \proglang{C++} framework for
sampling and optimization using the log probability and derivative
information supplied by a model.

\subsection{Markov chain Monte Carlo samplers}

\subsubsection{Hamiltonian Monte Carlo}

The MCMC samplers provided include Euclidean Manifold Hamiltonian
Monte Carlo (EM-HMC) \citep{DuaneEtAl:1987, Neal:1994, Neal:2011} and
the No-U-Turn sampler (NUTS) \citep{HoffmanGelman:2011}.  Both
versions of HMC allow estimation or specification of unit, diagonal,
or full mass matrices.  NUTS, the default sampler for \proglang{Stan},
automatically adapts the number of leapfrog steps, eliminating the 
need for user-specified tuning parameters.  Both algorithms take advantage
of curvature information, namely the gradient, to generate coherent motion
through the posterior that dramatically reduces the autocorrelation of
the resulting transitions.

\proglang{Stan} will soon have an implementation of Riemannian Manifold
Hamiltonian Monte Carlo (RM-HMC) \citep{GirolamiCalderhead:2011},
using the SoftAbs metric to robustly incorporate posterior curvature
into the Hamiltonian simulation \cite{Betancourt:2012}, although
\proglang{Stan}'s MCMC framework makes it
straightforward to incorporate other metrics.  This
implementation will also generalize NUTS to Riemannian manifolds
\citep{Betancourt:2013}.  Both support adapting step sizes during
warmup.  RM-HMC uses first, second, and third order derivatives 
to adapt to the local curvature of the posterior.

\subsubsection{Metropolis-Hastings}

\proglang{Stan} supports random-walk Metropolis-Hastings samplers with
multivariate normal proposals.  The covariance matrix used can be
adapted during warmup, assuming either equal diagonal covariance,
diagonal covariance, or a full covariance matrix.  Metropolis-Hastings
does not require any derivatives of the log probability function, but lack
of gradient information induces random walk behavior.


\subsubsection{Ensemble Samplers}

Two flavors of ensemble samplers are also in the works: an affine invariant
ensemble sampler \citep{GoodmanWeare:2010} and a differential
evolution sampler \cite{TerBraak:2006}.  Ensemble samplers do not
require derivative information, but typically require at least as many
chains to be run as the number of dimensions in order to have full
rank search of the posterior.

\subsection{Optimizers}

\subsubsection{BFGS}

The default optimizer in \proglang{Stan} is the
Broyden-Fletcher-Goldfarb-Shannon-Boyd (BFGS) optimizer.  BFGS is a
quasi-Newton optimizer that evaluates gradients directly, then uses
the gradients to update an approximation to the Hessian; see
\citep{NocedalWright:2006} for details.  Plans are in the works to
also include the more involved, but more scalable limited-memory BFGS
(L-BFGS) scheme.

\subsubsection{Conjugate gradient}

\proglang{Stan} provides a standard form of conjugate gradient
optimization; see \citep{NocedalWright:2006}.  As its name implies,
conjugate gradient optimization requires gradient evaluations.

\subsubsection{Dual averaging}

The dual averaging scheme of \cite{Nesterov:2009} is included for
model optimization.  Dual averaging is also used during warmup to
estimate covariance matrices for HMC and Metropolis sampling.  Dual
averaging uses the gradient of the log probabilty function.


\section{Random number generation}\label{rng.section}

Random number generation for \proglang{Stan} is done on a per-chain
basis; ensemble samplers form a single joint chain over the ensemble.  By
specifying the chain being used, the random number generator can be
skipped ahead sufficiently to avoid replication of subsequences of
random numbers across chains.

The generated model code and underlying \proglang{C++} algorithms
provide template parameters for a class implementing the \pkg{Boost}
random number generator concept.  

By default, \proglang{Stan} uses linear congruential generators
\citep{LEcuyer:1988}.  This generator supports efficient skip-ahead.

\subsection{Replicability}

The \proglang{Stan} interfaces all allow random-number generator seeds
to be specified explicitly.  Execution uses a single base random
number generator instance.  Therefore, by specifying a seed,
\proglang{Stan}'s behavior is deterministic.  This is very useful for
debugging purposes.  Seeds can be generated randomly based on
properties of the system time, but when they are, the seed used is
printed as part of the output to allow it to be fully replicated.


\section{Library dependencies}

\proglang{Stan}'s modeling language is only dependent on two external
libraries.  

\subsection{Boost} 

\proglang{Stan} depends on several of the \pkg{Boost} \proglang{C++}
libraries \citep{Boost:2011}.  \proglang{Stan} makes extensive use of
\pkg{Boost}'s template metaprogramming facilities including the
\pkg{Enable if} package, the \pkg{Type Traits} library, and the
\pkg{Lexical Cast} library.  The \proglang{Stan} language is parsed
using \pkg{Boost}'s \pkg{Spirit} parser, which itself depends on
the binding libraries \pkg{Phoenix}, \pkg{Bind}, and \pkg{Lambda},
the variant type library \pkg{Variant}, and the container library
\pkg{Fusion}.  Exceptions are largely handled and configured through
the error checking facilities in the \pkg{Math} and
\pkg{Exception} packages.  Output formatting and ad-hoc input parsing
for various formats is facilitated with the \pkg{Format} library.
\proglang{Stan} relies heavily on the special functions defined in the
\pkg{Math} subpackages \pkg{Special Functions} and \pkg{Statistical
  Distributions}.  Random number generation is carried out using the
\pkg{Random} package.  The posterior analysis framework and some
built-in functions depend on the \pkg{Accumulators} package.

\subsection{Eigen}

\proglang{Stan}'s handling of matrices and linear algebra is
implemented through the \pkg{Eigen} \proglang{C++} template library
\citep{Eigen:2012}.  Eigen uses template metaprogramming to achieve
state-of-the-art performance for matrix and linear algebra operations
with a great deal of flexiblity with respect to input types.
Unfortunately, many of the expression templates that Eigen uses for
efficient static anaysis and lazy evaluation are short-circuited
because of \proglang{Stan}'s need to have mixed type operations (i.e.,
multiplying a constant predictor matrix of double values by a
parameter vector of algorithmic differentiation values).  To make up for
this in some important cases, \proglang{Stan} has provided compound
functions such as the quadratic form, which allow speedups of both the
matrix operations and their derivatives compared to a direct
implementation using \proglang{Stan}'s built-in operators.


\section{Developer process}

\subsection{Version control and source repository}

\proglang{Stan}'s source code is hosted on GitHub and managed using
the \pkg{Git} version control system \citep{Chacon:2009}.  To manage
the workflow with so many developers working at any given time, the
project follows the GitFlow process \citep{Driessen:2010}.  All
developer submissions are managed through pull requests and we have
gratefully received patches from numerous sources outside the core
development team.


\subsection{Continuous integration}

\proglang{Stan} uses continuous integration, meaning that the entire
program and set of tests are run automatically as code is pushed to
the \pkg{Git} repository.  Each pull request is tested for
compatibility with the development branch, and the development branch
itself is tested for stability.  \proglang{Stan} uses Jenkins
\citep{Smart:2011}, an open-source continuous integration server.

\subsection{Testing framework}

This includes extensive unit tests for low-level \proglang{C++} code.
Unit tests are implemented using the \pkg{googletest} framework
\citep{GoogleTest:2011}.  The probability functions and command-line
invocations are complex enough that programs are used to automatically
generate test code for \pkg{googletest}.

These unit tests evaluate every function for both appropriate values
and appropriate derivatives.  This requires an extensive meta-testing
framework for the probability distributions due to their high degree
of configurability as to argument types.  The testing portion of the
makefile also runs tests of all of the built-in models, including
almost all of the \proglang{BUGS} sample models.  Models are tested
for both convergence and posterior mean estimation to within MCMC
standard error.


\subsection{Builds}

The build process for \proglang{Stan} is highly automated through a
cross-platform series of \code{make} files.  The top-level makefile
builds the \proglang{Stan}-to-\proglang{C++} translator command
\code{bin/stanc} and posterior analysis command \code{bin/print}.  It
also builds the library archive \code{bin/libstan.a}.  Great care was
taken to avoid complicated platform-dependent configuration
requirements that place a high burden on user system knowledge for
installation.  All that is needed is a relatively recent
\proglang{C++} compiler and version of \code{make}.

As exemplified in the introduction, the makefile is automated enough
to build an executable form of a \proglang{Stan} model in a single
command.  All libraries and other executables will be built as a side
effect.  

The top-level makefile also supplies targets to build all of the
documentation \proglang{C++} API documentation is generated using the
\pkg{doxygen} package \citep{Doxygen:2011}.  The \proglang{Stan}
manual \citep{Stan:2013} is typeset using the \LaTeX package
\citep{MittelbachEtAl:2004}.

The makefile also has targets for all of the unit and functional
testing, for building the source-level distribution, and for cleaning
any temporary files that it creates.

\section*{Acknowledgments}

First and foremost, we would like to thank all of the users of
\proglang{Stan} for taking a chance on a new package and sticking with
it as we ironed out the details in the first release.  We'd like to
particularly single out the students in Andrew Gelman's Bayesian data
analysis courses at Columbia Univesity and Harvard University, who
served as trial subjects for both \proglang{Stan} and \cite{GelmanEtAl:2013}.

Stan was and continues to be supported largely through grants from the
U.~S. government.  Grants which indirectly supported the initial
research and development included grants from the Department of Energy
(DE-SC0002099), the National Science Foundation (ATM-0934516), and the
Department of Education Institute of Education Sciences
(ED-GRANTS-032309-005 and R305D090006-09A).  The high-performance
computing facility on which we ran evaluations was made possible
through a grant from the National Institutes of Health
(1G20RR030893-01).  Stan is currently supported in part by a grant
from the National Science Foundation (CNS-1205516).

Finally, we would like to thank all those who contributed
documentation corrections and code patches, Jeffrey Arnold, David
R. Blair, Eric C. Brown, Eric N. Brown, Devin Caughey, Wayne Folta,
Andrew Hunter, Marco Inacio, Louis Luangkesorn, Jeffrey Oldham, Mike
Ross, Terrance Savitsky, Yajuan Si, Dan Stowell, Zhenming Su, and
Dougal Sutherland.

And a special thanks to those who have contributed code for new
features, Jeffrey Arnold, Yuanjun Gao, and Marco Inacio. 

\begin{figure}
\begin{center}
\begin{tabular}{c|ccl|l}
{\it Operation} & {\it Precedence} & {\it Associativity} & {\it
  Placement} & {\it Description}
\\ \hline \hline
\code{||} & 9 & left & binary infix & logical or
\\ \hline
\Verb|&&| & 8 & left & binary infix & logical and
\\ \hline
\Verb|==| & 7 & left & binary infix & equality
\\
\Verb|!=| & 7 & left & binary infix & inequality
\\ \hline
\Verb|<| & 6 & left & binary infix & less than
\\
\Verb|<=| & 6 & left & binary infix & less than or equal
\\
\Verb|>| & 6 & left & binary infix & greater than 
\\
\Verb|>=| & 6 & left & binary infix & greater than or equal
\\ \hline
\code{+} & 5 & left & binary infix & addition
\\
\code{-} & 5 & left & binary infix & subtraction
\\ \hline
\code{*} & 4 & left & binary infix & multiplication
\\
\code{/} & 4 & left & binary infix & (right) division
\\ \hline
\Verb|\| & 3 & left & binary infix & left division
\\ \hline
\code{.*} & 2 & left & binary infix & elementwise multiplication
\\
\code{./} & 2 & left & binary infix & elementwise division
\\ \hline
\code{!} & 1 & n/a & unary prefix & logical negation
\\
\code{-} & 1 & n/a & unary prefix & negation
\\ 
\code{+} & 1 & n/a & unary prefix & promotion (no-op in \proglang{Stan})
\\ \hline
\code{'} & 0 & n/a & unary postfix & transposition
\\ \hline \hline
\code{()} & 0 & n/a & prefix, wrap & function application
\\
\code{[]} & 0 & left & prefix, wrap & array, matrix indexing
\end{tabular}
\end{center}
\caption{Each of \proglang{Stan}'s unary and binary operators follow
  strict precedences, associativities, placement within an expression.  
  The operators are listed in order of precedence, from least tightly
  binded to most tightly binding.}\label{operators.fig}
\end{figure}
%


\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Function} & {\it Description} \\ \hline \hline
\code{e} &  base of natural logarithm \\ 
\code{epsilon} &  smallest positive number \\ 
\code{negative\_epsilon} &  largest negative value \\ 
\code{negative\_infinity} &  negative infinity \\ 
\code{not\_a\_number} &  not-a-number \\ 
\code{pi} &  $\pi$ \\
\code{positive\_infinity} &  positive infinity \\  
\code{sqrt2} &  square root of two \\ 
\end{tabular}
\end{center}
\caption{\proglang{Stan} implements a variety of useful constants.}\label{constants.fig}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Function} & {\it Description} \\ \hline \hline
\code{acos} &  arc cosine \\ 
\code{acosh} &  arc hyperbolic cosine \\ 
\code{asin} &  arc sine \\ 
\code{asinh} &  arc hyperbolic sine \\ 
\code{atan} &  arc tangent \\ 
\code{atan2} &  arc ratio tangent \\ 
\code{atanh} &  arc hyperbolic tangent \\ 
\code{cos} &  cosine \\ 
\code{cosh} &  hyperbolic cosine\\ 
\code{hypot} &  hypoteneuse \\ 
\code{sin} &  sine \\ 
\code{sinh} &  hyperbolic sine \\ 
\code{tan} &  tangent \\ 
\code{tanh} &  hyperbolic tangent \\ 
\end{tabular}
\end{center}
\caption{\proglang{Stan} implements both circular and 
  hyperbolic trigonometric functions, as well as their inverses.}
  \label{trig-functions-cont.fig}
\end{figure}


\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Function} & {\it Description}  \\ \hline \hline
\code{abs} &  double absolute value \\ 
\code{abs} &  integer absolute value \\ 
\code{binary\_log\_loss} &  log loss \\ 
\code{bessel\_first\_kind} & Bessel function of the first kind \\
\code{bessel\_second\_kind} & Bessel function of the second kind \\
\code{binomial\_coefficient\_log} & log binomial coefficient \\ 
\code{cbrt} &  cube root \\ 
\code{ceil} &  ceiling \\ 
\code{cumulative\_sum} &  cumulative sum \\ 
\code{erf} &  error function \\ 
\code{erfc} &  complementary error function \\ 
\code{exp} &  base-$e$ exponential \\ 
\code{exp2} &  base-2 exponential \\ 
\code{expm1} &  exponential of quantity minus one \\
\code{fabs} &  real absolute value \\ 
\code{fdim} &  positive difference \\ 
\code{floor} &  floor \\ 
\code{fma} &  fused multiply-add \\ 
\code{fmax} &  floating-point maximum \\ 
\code{fmin} &  floating-point minimum \\ 
\code{fmod} &  floating-point modulus \\ 
\code{if\_else} &  conditional \\ 
\code{int\_step} &  Heaviside step function \\ 
\code{inv} &  inverse (one over argument) \\ 
\code{inv\_cloglog} &  inverse of complementary log-log \\ 
\code{inv\_logit} &  logistic sigmoid \\ 
\code{inv\_sqrt} &  inverse square root \\ 
\code{inv\_square} &  inverse square \\ 
\code{lbeta} & log beta function \\ 
\code{lgamma} &  log $\Gamma$ function \\ 
\code{lmgamma} &  log multi-$\Gamma$ function \\ 
\code{log} &  natural (base-$e$) logarithm \\ 
\code{log10} &  base-10 logarithm \\ 
\code{log1m} &  natural logarithm of one minus argument \\ 
\code{log1m\_exp} &  natural logarithm of one minus natural exponential \\ 
\code{log1m\_inv\_logit} &  natural logarithm of logistic sigmoid \\ 
\code{log1p} &  natural logarithm of one plus argument \\ 
\code{log1p\_exp} &  natural logarithm of one plus natural exponential \\ 
\code{log2} &  base-2 logarithm \\ 
\end{tabular}
\end{center}
\caption{\proglang{Stan} implements many special and transcendental functions.}\label{special-functions.fig}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Function} & {\it Description} \\ \hline \hline
\code{log\_diff\_exp} & natural logarithm of difference of
exponentials \\ 
\code{log\_falling\_factorial} & falling factorial (Pochhammer) \\
\code{log\_inv\_logit} &  natural logarithm of the logistic sigmoid \\ 
\code{log\_rising\_factorial} & falling factorial (Pochhammer) \\
\code{log\_sum\_exp} &  logarithm of the sum of exponentials of arguments \\ 
\code{logit} &  log-odds \\ 
\code{max} &  integer maximum \\ 
\code{max} &  real maximum \\ 
\code{mean} &  sample average \\ 
\code{min} &  integer minimum \\ 
\code{min} &  real minimum \\ 
\code{modified\_bessel\_first\_kind} & modified Bessel function of the first kind \\
\code{modified\_bessel\_second\_kind} & modified Bessel function of the second kind \\
\code{multiply\_log} &  multiply linear by log \\ 
\code{owens\_t} &  Owens-t \\ 
\code{phi} &  $\Phi$ function (cumulative unit normal) \\ 
\code{phi\_approx} &  efficient, approximate $\Phi$ \\ 
\code{pow} &  power (i.e., exponentiatiation) \\
\code{prod} &  product of sequence \\
\code{rank} &  rank of element in array or vector \\ 
\code{rep\_array} &  fill array with value \\
\code{round} &  round to nearest integer \\ 
\code{sd} &  sample standard deviation \\ 
\code{softmax} &  softmax (multi-logit link) \\ 
\code{sort\_asc} &  sort in ascending order \\ 
\code{sort\_desc} &  sort in descending order \\ 
\code{sqrt} &  square root \\ 
\code{square} &  square \\ 
\code{step} &  Heaviside step function \\ 
\code{sum} &  sum of sequence \\ 
\code{tgamma} &  $\Gamma$ function \\ 
\code{trunc} & truncate real to integer \\ 
\code{variance} &  sample variance \\ 
\end{tabular}
\end{center}
\caption{Special functions (continued).}\label{special-functions-cont.fig}
\end{figure}



\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Function} & {\it Description} \\ \hline \hline
\code{block} &  sub-block of matrix \\
\code{cholesky\_decompose} &  Cholesky decomposition \\ 
\code{col} &  column of matrix \\ 
\code{cols} &  number of columns in matrix \\ 
\code{columns\_dot\_product} &  dot product of matrix columns \\ 
\code{columns\_dot\_self} &  dot product of matrix columns with self \\ 
\code{crossprod} &  cross-product \\ 
\code{determinant} &  matrix determinant \\ 
\code{diag\_matrix} &  vector to diagonal matrix \\ 
\code{diag\_post\_multiply} &  post-multiply matrix by diagonal matrix \\ 
\code{diag\_post\_multiply} &  pre-multiply matrix by diagonal matrix \\ 
\code{diagonal} &  diagonal of matrix as vector \\ 
\code{dims} &  dimensions of matrix \\ 
\code{dot\_product} &  dot product \\ 
\code{dot\_self} &  dot product with self \\ 
\code{eigenvalues\_sym} &  eigenvalues of symmetric matrix \\ 
\code{eigenvectors\_sym} &  eigenvectors of symmetric matrix \\ 
\code{head} &  head of vector \\
\code{inverse} &  matrix inverse \\ 
\code{inverse\_spd} & symmetric, positive-definite matrix inverse \\ 
\code{log\_determinant} &  natural logarithm of determinant \\ 
\code{mdivide\_left\_tri\_low} &  lower-triangular matrix left division \\ 
\code{mdivide\_right\_tri\_low} &  lower-triangular matrix right division \\
\code{multiply\_lower\_tri\_self\_transpose} &  multiply
lower-triangular by transpose \\ 
\code{quad\_form} &  quadratic form vector-matrix multiplication \\ 
\code{rep\_matrix} &  replicate scalar, row vector or vector to matrix \\ 
\code{rep\_row\_vector} &  replicate scalar to row vector \\ 
\code{rep\_vector} &  replicate scalar to vector \\ 
\code{row} &  row of matrix \\ 
\code{rows} &  number of rows in matrix \\ 
\code{rows\_dot\_product} &  dot-product of rows of matrices \\ 
\code{rows\_dot\_self} &  dot-product of matrix with itself \\ 
\code{segment} &  sub-vector \\ 
\code{singular\_values} &  singular values of matrix \\ 
\code{size} &  number of entries in array or vector \\ 
\code{sub\_col} &  sub-column of matrix \\ 
\code{sub\_row} &  sub-row of matrix \\ 
\code{tail} &  tail of vector \\ 
\code{tcrossprod} &  matrix post-multiply by own transpose \\ 
\code{trace} &  trace of matrix \\ 
\code{trace\_gen\_quad\_form} & trace of generalized quadratic form \\ 
\code{trace\_quad\_form} &  trace of quadratic form
\end{tabular}
\end{center}
\caption{A large suite of matrix functions admits efficient 
  multivariate model implementation in \proglang{Stan}.}
  \label{matrix-functions.fig}
\end{figure}


\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Function} & {\it Description} \\ \hline \hline
\code{bernoulli\_cdf} &  Bernoulli cdf \\ 
\code{bernoulli\_log} &  log Bernoulli pmf \\ 
\code{bernoulli\_logit\_log} &  logit-scale log Bernoulli pmf \\ 
\code{bernoulli\_rng} &  Bernoulli RNG \\ 
\code{beta\_binomial\_cdf} &  beta-binomial cdf \\ 
\code{beta\_binomial\_log} &  log beta-binomial pmf \\ 
\code{beta\_binomial\_rng} &  beta-binomial rng \\ 
\code{beta\_cdf} &  beta cdf \\ 
\code{beta\_log} &  log beta pdf \\ 
\code{beta\_rng} &  beta RNG \\ 
\code{binomial\_cdf} &  binomial cdf n \\ 
\code{binomial\_log} &  log binomial pmf \\ 
\code{binomial\_logit\_log} &  log logit-scaled binomial pmf \\ 
\code{binomial\_rng} &  binomail RNG \\ 
\code{categorical\_log} &  log categorical pmf \\ 
\code{categorical\_rng} &  categorical RNG \\ 
\code{cauchy\_cdf} &  Cauchy cdf \\ 
\code{cauchy\_log} &  log Cauchy pdf \\ 
\code{cauchy\_rng} &  Cauchy RNG \\ 
\code{chi\_square\_log} &  log chi-square pdf \\ 
\code{chi\_square\_rng} &  chi-square RNG \\ 
\code{dirichlet\_log} &  log Dirichlet pdf \\ 
\code{dirichlet\_rng} &  Dirichlet RNG \\ 
\code{double\_exponential\_log} &  log double-exponential (Laplace) pdf \\ 
\code{double\_exponential\_rng} &  double-exponential (Laplace) RNG \\ 
\code{exp\_mod\_normal\_cdf} &  exponentially modified normal cdf \\
\code{exp\_mod\_normal\_log} &  log exponentially modified normal pdf \\ 
\code{exp\_mod\_normal\_rng} &  exponentially modified normal RNG \\ 
\code{exponential\_cdf} &  exponentia cdf \\ 
\code{exponential\_log} &  log of exponential pdf \\ 
\code{exponential\_rng} &  exponential RNG \\ 
\code{gamma\_log} &  log gamma pdf \\ 
\code{gamma\_rng} &  gamma RNG \\ 
\code{gumbel\_cdf} &  Gumbel cdf \\ 
\code{gumbel\_log} &  log Gumbel pdf \\ 
\code{gumbel\_rng} &  Gumbel RNG\\ 
\code{hypergeometric\_log} &  log hypergeometric pmf \\ 
\code{hypergeometric\_rng} &  hypergeometric RNG \\ 
\code{inv\_chi\_square\_cdf} &  inverse chi-square cdf \\ 
\code{inv\_chi\_square\_log} &  log inverse chi-square pdf \\ 
\code{inv\_chi\_square\_rng} &  inverse chi-square RNG \\ 
\end{tabular}
\end{center}
\caption{Most common probability distributions have
  explicitly implemented in \proglang{Stan}.}\label{prob-functions.fig}
\end{figure}

\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Function} & {\it Description} \\ \hline \hline
\code{inv\_gamma\_cdf} & inverse gamma cdf \\ 
\code{inv\_gamma\_log} &  log inverse gamma pdf \\ 
\code{inv\_gamma\_rng} &  inverse gamma RNG \\ 
\code{inv\_wishart\_log} &  log inverse Wishart pdf \\ 
\code{inv\_wishart\_rng} &  inverse Wishart RNG \\ 
\code{lkj\_corr\_cholesky\_log} &  log LKJ correlation, Cholesky-variate pdf \\ 
\code{lkj\_corr\_cholesky\_rng} &  LKJ correlation, Cholesky-variate RNG \\
\code{lkj\_corr\_log} &  log of LKJ correlation pdf \\ 
\code{lkj\_corr\_rng} &  LKJ correlation RNG \\ 
% \code{lkj\_cov\_log} &  {\it deprecated} \\ 
\code{logistic\_cdf} &  logistic cdf \\ 
\code{logistic\_log} &  log logistic pdf \\ 
\code{logistic\_rng} &  logistic RNG \\ 
\code{lognormal\_cdf} &  lognormal cdf \\ 
\code{lognormal\_log} &  log of lognormal pdf \\ 
\code{lognormal\_rng} &  lognormal RNG \\ 
\code{multi\_normal\_cholesky\_log} & log multi-normal Cholesky-parameterized pdf  \\ 
\code{multi\_normal\_log} &  log multi-normal pdf \\ 
\code{multi\_normal\_prec\_log} & log multi-normal precision-parameterized pdf \\ 
\code{multi\_normal\_rng} &  multi-normal RNG \\ 
\code{multi\_student\_t\_log} & log multi student-t pdf \\
\code{multi\_student\_t\_rng} &  multi student-t RNG \\ 
\code{multinomial\_log} &  log multinomial pmf \\ 
\code{multinomial\_rng} &  multinomial RNG \\ 
\code{neg\_binomial\_cdf} &  negative binomial cdf \\ 
\code{neg\_binomial\_log} &  log negative binomial pmf \\ 
\code{neg\_binomial\_rng} &  negative biomial RNG \\ 
\code{normal\_cdf} &  normal cdf \\ 
\code{normal\_log} &  log normal pdf (c.f. log lognormal pdf) \\ 
\code{normal\_rng} &  normal RNG \\ 
\code{ordered\_logistic\_log} &  log ordinal logistic pmf \\ 
\code{ordered\_logistic\_rng} &  ordinal logistic RNG \\ 
\code{pareto\_cdf} &  Pareto cdf \\ 
\code{pareto\_log} &  log Pareto pdf \\ 
\code{pareto\_rng} &  Pareto RNG \\ 
\code{poisson\_cdf} &  Poisson cdf \\ 
\code{poisson\_log} &  log Poisson pmf \\ 
\code{poisson\_log\_log} &  log Poisson log-parameter pdf \\ 
\code{poisson\_rng} &  Poisson RNG \\ 
\end{tabular}
\end{center}
\caption{Probability functions (continued)}\label{prob-functions-cont.fig}
\end{figure}


\begin{figure}
\begin{center}
\begin{tabular}{l|l}
{\it Function} & {\it Description} \\ \hline \hline
\code{rayleigh\_log} & log Rayleigh pdf \\
\code{rayleigh\_rng} & Rayleigh RNG \\
\code{rayleigh\_cdf} & Rayleigh cdf \\
\code{scaled\_inv\_chi\_square\_cdf} &  scaled inverse-chi-square cdf \\ 
\code{scaled\_inv\_chi\_square\_log} &  log scaled inverse-chi-square pdf \\ 
\code{scaled\_inv\_chi\_square\_rng} &  scaled inverse-chi-square RNG \\ 
\code{skew\_normal\_cdf} &  skew-normal cdf \\
\code{skew\_normal\_log} &  log of skew-normal pdf \\
\code{skew\_normal\_rng} &  skew-normal RNG \\ 
\code{student\_t\_cdf} &  Student-t cdf \\
\code{student\_t\_log} &  log of Student-t pdf \\
\code{student\_t\_rng} &  Student-t RNG \\ 
\code{uniform\_log} &  log of uniform pdf \\ 
\code{uniform\_rng} &  uniform RNG \\ 
\code{weibull\_cdf} &  Weibull cdf \\ 
\code{weibull\_log} &  log of Weibull pdf \\ 
\code{weibull\_rng} &  Weibull RNG \\ 
\code{wishart\_log} &  log of Wishart pdf \\ 
\code{wishart\_rng} &  Wishart RNG \\ 
\end{tabular}
\end{center}
\caption{Probability functions (continued 2)}\label{prob-functions-cont-2.fig}
\end{figure}


% \nocite{WainwrightJordan:2008}
\nocite{R:2013}
\clearpage
%\bibliographystyle{jss}
\bibliography{stan-paper}
























 




\end{document}

