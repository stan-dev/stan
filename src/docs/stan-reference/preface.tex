\chapter*{Preface}
\addcontentsline{toc}{chapter}{Preface}

\section*{Why \Stan?}

We did not set out to build \Stan as it currently exists.  We set out to
apply full Bayesian inference to the sort of multilevel generalized
linear models discussed in Part II of \citep{GelmanHill:2007}.  These
models are structured with grouped and interacted predictors at
multiple levels, hierarchical covariance priors, nonconjugate
coefficient priors, latent effects as in item-response models, and
varying output link functions and distributions.

The models we wanted to fit turned out to be a challenge for current
general-purpose software.  A direct encoding in \BUGS or \JAGS can
grind these tools to a halt.  Matt Schofield found his multilevel
time-series regression of climate on tree-ring measurements wasn't
converging after hundreds of thousands of iterations.

Initially, Aleks Jakulin spent some time working on extending the
Gibbs sampler in the Hierarchical Bayesian Compiler
\citep{DaumeIII:2007}, which as its name suggests, is compiled rather
than interpreted.  But even an efficient and scalable implementation
does not solve the underlying problem that Gibbs sampling does not
fare well with highly correlated posteriors.  We finally realized we
needed a better sampler, not a more efficient implementation.

We briefly considered trying to tune proposals for a random-walk
Metropolis-Hastings sampler, but that seemed too problem specific and
not even necessarily possible without some kind of adaptation rather
than tuning of the proposals. 
 

\section*{The Path to \Stan}

We were at the same time starting to hear more and more about
Hamiltonian Monte Carlo (\HMC) and its ability to overcome some of the
the problems inherent in Gibbs sampling.  Matt Schofield managed to
fit the tree-ring data using a hand-coded implementation of \HMC,
finding it converged in a few hundred iterations.

\HMC appeared promising but was also problematic in that the
Hamiltonian dynamics simulation requires the gradient of the log
posterior.  Although it's possible to do this by hand, it is very
tedious and error prone.  That's when we discovered reverse-mode
algorithmic differentiation, which lets you write down a templated
\Cpp function for the log posterior and automatically compute a proper
analytic gradient up to machine precision accuracy in only a few
multiples of the cost to evaluate the log probability function itself.
We explored existing algorithmic differentiation packages with open
licenses such as {\sc rad} \citep{Gay:2005} and its repackaging in the
Sacado module of the Trilinos toolkit and the {\small CppAD} package in the
{\sc coin-or} toolkit.  But neither package supported very many
special functions (e.g., probability functions, log gamma, inverse logit) or
linear algebra operations (e.g., Cholesky decomposition) and were not
easily and modularly extensible.  

So we built our own reverse-mode algorithmic differentiation package.
But once we'd built our own reverse-mode algorithmic differentiation
package, the problem was that we could not just plug in the
probability functions from a package like Boost because they weren't
templated on all the arguments.  We only needed algorithmic
differentiation variables for parameters, not data or transformed
data, and promotion is very inefficient in both time and memory.  So
we wrote our own fully templated probability functions.  

Next, we integrated the Eigen \Cpp package for matrix operations and
linear algebra functions.  Eigen makes extensive use of expression
templates for lazy evaluation and the curiously recurring template
pattern to implement concepts without virtual function calls.  But we
ran into the same problem with Eigen as with the existing probability
libraries --- it doesn't support mixed operations of algorithmic
differentiation variables and primitives like \code{double}.

At this point (Spring 2011), we were happily fitting models coded 
directly in \Cpp on top of the pre-release versions of the \Stan API. 
Seeing how well this all worked, we set our sights on the generality 
and ease of use of \BUGS.  So we designed a modeling language in which 
statisticians could write their models in familiar notation that could 
be transformed to efficient \Cpp code and then compiled into an 
efficient executable program.  It turned out that our modeling
language was a bit more general than we'd anticipated, and we had an
imperative probabilistic programming language on our hands.%
%
\footnote{In contrast, BUGS and JAGS can be viewed as declarative
  probabilistic programming languages for specifying a directed
  graphical model.  In these languages, stochastic and deterministic
  (poor choice of name) nodes may represent random quantities.}
 
The next problem we ran into as we started implementing richer models
is variables with constrained support (e.g., simplexes and covariance
matrices).  Although it is possible to implement \HMC with bouncing
for simple boundary constraints (e.g., positive scale or precision
parameters), it's not so easy with more complex multivariate
constraints.  To get around this problem, we introduced typed
variables and automatically transformed them to unconstrained support
with suitable adjustments to the log probability from the log absolute
Jacobian determinant of the inverse transforms.

Even with the prototype compiler generating models, we still faced a
major hurdle to ease of use.  \HMC requires a step size
(discretization time) and number of steps (for total simulation time),
and is very sensitive to how they are set.  The step size parameter
could be tuned during warmup based on Metropolis rejection rates, but
the number of steps was not so easy to tune while maintaining detailed
balance in the sampler.  This led to the development of the No-U-Turn
sampler (\NUTS) \citep{Hoffman-Gelman:2011, Hoffman-Gelman:2014},
which takes an exponentially increasing number of steps (structured as
a binary tree) forward and backward in time until the direction of the
simulation turns around, then uses slice sampling to select a point on
the simulated trajectory.

Although not part of the original Stan prototype, which used a unit
mass matrix, Stan now allows a diagonal or dense mass matrix to be
estimated during warmup.  This allows adjustment for globally scaled
or correlated parameters.  Without this adjustment, models with
differently scaled parameters could only mix as quickly as their most
constrained parameter allowed.

We thought we were home free at this point.  But when we measured the
speed of some \BUGS examples versus \Stan, we were very disappointed.
The very first example model, Rats, ran more than an order of
magnitude faster in \JAGS than in \Stan.  Rats is a tough test case
because the conjugate priors and lack of posterior correlations make
it an ideal candidate for efficient Gibbs sampling.  But we thought
the efficiency of compilation might compensate for the lack of ideal
fit to the problem.  

We realized we were doing redundant calculations, so we wrote a
vectorized form of the normal distribution for multiple variates with
the same mean and scale, which sped things up a bit. At the same time,
we introduced some simple template metaprograms to remove the
calculation of constant terms in the log probability.  These both
improved speed, but not enough.  Finally, we figured out how to both
vectorize and partially evaluate the gradients of the densities using
a combination of expression templates and metaprogramming.  At this
point, we are within a small multiple of a hand-coded gradient
function.

Later, when we were trying to fit a time-series model, we found that
normalizing the data to unit sample mean and variance sped up the fits
by an order of magnitude.  Although \HMC and \NUTS are rotation
invariant (explaining why they can sample effectively from
multivariate densities with high correlations), they are not scale
invariant.  Gibbs sampling, on the other hand, is scale invariant, but
not rotation invariant.

We were still using a unit mass matrix in the simulated Hamiltonian
dynamics.  The last tweak to \Stan before version 1.0 was to estimate
a diagonal mass matrix during warmup; this has since been upgraded to
a full mass matrix in version 1.2.  Both these extensions go a bit
beyond the \NUTS paper on {\it arXiv}.  Using a mass matrix sped up
the unscaled data models by an order of magnitude, though it breaks
the nice theoretical property of rotation invariance.  The full mass
matrix estimation has rotational invariance as well, but scales less
well because of the need to invert the mass matrix at the end of
adaptation blocks and then perform matrix multiplications every
leapfrog step.

\section*{Stan 2}

It's been over a year since the initial release of Stan, and we have
been overjoyed by the quantity and quality of models people are
building with Stan.  We've also been a bit overwhelmed by the volume
of traffic on our user's list and issue tracker.

We've been particularly happy about all the feedback we've gotten
about installation issues as well as bugs in the code and
documentation.  We've been pleasantly surprised at the number of such
requests which have come with solutions in the form of a GitHub pull
request.  That certainly makes our life easy.

As the code base grew and as we became more familiar with it, we came
to realize that it required a major refactoring (see, for example,
\citep{FowlerEtAl:1999} for a nice discussion of refactoring).  So
while the outside hasn't changed dramatically in Stan 2, the inside is
almost totally different in terms of how the HMC samplers are
organized, how the output is analyzed, how the mathematics library is
organized, etc.  

We've also improved our original simple optimization algorithms and
now use L-BFGS (a limited memory quasi-Newton method that uses
gradients and a short history of the gradients to make a rolling
estimate of the Hessian).

We've added more compile-time and run-time error checking for models.
We've added many new functions, including new matrix functions and new
distributions.  We've added some new parameterizations and managed to
vectorize all the univariate distributions.  We've increased
compatibility with a range of C++ compilers.

We've also tried to fill out the manual to clarify things like array
and vector indexing, programming style, and the I/O and command-line
formats.  Most of these changes are direct results of user-reported
confusions.  So please let us know where we can be clearer or more
fully explain something.

Finally, we've fixed all the bugs which we know about.  It was keeping
up with the latter that really set the development time back,
including bugs that resulted in our having to add more error checking.

Perhaps most importantly, we've developed a much stricter process for
unit testing, code review, and automated integration testing (see
\refchapter{software-development}).

As fast and scalable as Stan's MCMC sampling is, for large data sets
it can still be prohibitively slow.  Stan 2.7 introduced variational
inference for arbitrary Stan models.  In contrast to penalized maximum
likelihood, which finds the posterior mode, variational inference
finds an approximation to the posterior mean (both methods use
curvature to estimate a multivariate normal approximation to posterior
covariance).  This promises Bayesian inference at much larger scale than is
possible with MCMC methods.  In examples we've run, problems that take
days with MCMC complete in half an hour with variational inference.
There is still a long road ahead in understanding these variational
approximations, both in how good the multivariate approximation is to
the true posterior and which forms of models can be fit efficiently,
scalably, and reliably.


\section*{\Stan's Future}

We're not done. There's still an enormous amount of work to do to
improve \Stan.  Some older, higher-level goals are in a standalone
to-do list:
%
\begin{quote}
\url{https://github.com/stan-dev/stan/wiki/Longer-Term-To-Do-List}
\end{quote}

%
We are gradually weaning ourselves off of the to-do list in favor of
the GitHub issue tracker (see the next section for a link).

Some major features are on our short-term horizon: Riemannian manifold
Hamiltonian Monte Carlo (RHMC), transformed Laplace approximations
with uncertainty quantification for maximum likelihood estimation,
marginal maximum likelihood estimation, data-parallel expectation
propagation, and streaming (stochastic) variational inference.  The
latter has been prototyped and described in papers. 

We will also continue to work on improving numerical stability and
efficiency throughout.  In addition, we plan to revise the interfaces
to make them easier to understand and more flexible to use (a
difficult pair of goals to balance).

Later in the Stan 2 release cycle (Stan 2.7), we added variational
inference to Stan's sampling and optimization routines, with the
promise of approximate Bayesian inference at much larger scales than
is possible with Monte Carlo methods.  The future plans involve
extending to a stochastic data-streaming implementation for very
large-scale data problems.



\section*{You Can Help}

Please let us know if you have comments about this manual or
suggestions for \Stan.  We're especially interested in hearing about
models you've fit or had problems fitting with \Stan.  The best way to
communicate with the \Stan team about user issues is through the
following user's group.
%
\begin{quote}
\url{http://groups.google.com/group/stan-users}
\end{quote}
%
For reporting bugs or requesting features, \Stan's issue tracker is at
the following location.
%
\begin{quote}
\url{https://github.com/stan-dev/stan/issues}
\end{quote}

One of the main reasons \Stan is freedom-respecting, open-source
software%
%
\footnote{See \refappendix{licensing} for more information on Stan's
  licenses and the licenses of the software on which it depends.}
%
is that we love to collaborate.  We're interested in hearing
from you if you'd like to volunteer to get involved on the development
side.  We have all kinds of projects big and small that we haven't had
time to code ourselves.  For developer's issues, we have a separate
group.
%
\begin{quote}
\url{http://groups.google.com/group/stan-dev}
\end{quote}

To contact the project developers off the mailing lists, send email to
\begin{quote}
\href{mailto:mc.stanislaw@gmail.com}{\nolinkurl{mc.stanislaw@gmail.com}}
\end{quote}

\vspace*{12pt}
\mbox{ } \hfill {\it The \Stan Development Team}
\\
\mbox{ } \hfill \today
